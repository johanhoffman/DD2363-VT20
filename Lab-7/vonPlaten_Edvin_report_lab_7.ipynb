{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vonPlaten_Edvin-report-lab-7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81x9istK3zrj",
        "colab_type": "text"
      },
      "source": [
        "[**OPEN IN COLAB**](https://colab.research.google.com/drive/1tVXUROGgvxgcwfcV1E1eRt0fZuPihkLi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7",
        "colab_type": "text"
      },
      "source": [
        "# **Lab 7: Optimization**\n",
        "**Edvin von Platen**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm",
        "colab_type": "text"
      },
      "source": [
        "# **Abstract**\n",
        "In this lab we implement and evaluate the following two methods for optimization in $R^n$:\n",
        "\n",
        "1. Gradient Descent\n",
        "2. Newton's Method\n",
        "\n",
        "The implementation of the two methods appear to be sound and they perfom quite well, especially Newton's Method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3",
        "colab_type": "text"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab_type": "code",
        "outputId": "f74fa781-413b-41e7-a2ec-1bba2288ad4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2020 Edvin von Platen\n",
        "\n",
        "# This file is part of the course DD2363 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh",
        "colab_type": "text"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa",
        "colab_type": "text"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "We implement and evaluate the following two algorithms for optimization in $R^n$:\n",
        "\n",
        "1. Gradient Descent\n",
        "2. Newton's Method\n",
        "\n",
        "All implementations and mathematical conccepts presented in this report are based on the lecture notes from the course [DD2363 Methods in Scientific Computing](https://kth.instructure.com/courses/17068). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeFO9QMeUOAu",
        "colab_type": "text"
      },
      "source": [
        "# **Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx",
        "colab_type": "text"
      },
      "source": [
        "### **Gradient Descent in $R^n$**\n",
        "Gradient descent is an iterative algorithm for finding a minumum of an object function $f(x)$. The idea is to search for the minima in the opposite direction of the gradient. We implement Algorithm 15.1 with step length $\\alpha^{(k)}$ satisfying,\n",
        "$$\n",
        "f(x^{(k)} - \\alpha^{(k)}\\nabla f(x^{(k)})) \\leq \\beta f(x^{(k)}),\n",
        "$$\n",
        "with $0 < \\beta < 1$ as a parameter. For the stopping criterion we use,\n",
        "$$\n",
        "\\Vert \\nabla f(x^{(k)}) \\Vert < TOL,\n",
        "$$\n",
        "A finite difference approximation of the gradient is computed in each step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TeRUvAFIcwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finite difference approximation\n",
        "def compute_gradient(f, x, h=0.05):\n",
        "  n = x.shape[0]\n",
        "  Df = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    x_tmp = np.copy(x)\n",
        "    x_tmp[i] = x_tmp[i] +  h\n",
        "    f1 = f(x_tmp)\n",
        "    x_tmp[i] =  x_tmp[i] - 2*h\n",
        "    f2 = f(x_tmp)\n",
        "    Df[i] = (f1 - f2)/(2.0*h)\n",
        "  return Df\n",
        "\n",
        "def get_step_length(f, Df, x, beta):\n",
        "  # With alpha = 1 the method fails to find minima for some functions.\n",
        "  alpha = 0.99\n",
        "  fx = f(x)\n",
        "  factor = 0.7\n",
        "  while f(x - alpha * Df) >= beta * fx and alpha > 0.005:\n",
        "    alpha = alpha * factor\n",
        "  return alpha\n",
        "\n",
        "def gradient_descent_method(f, x0, beta=0.5, h=0.05, TOL=0.001):\n",
        "  x = x0\n",
        "  Df = compute_gradient(f, x0, h)\n",
        "  iters = 0\n",
        "  while np.linalg.norm(Df) > TOL:\n",
        "    Df = compute_gradient(f, x, h)\n",
        "    alpha = get_step_length(f, Df, x, beta)\n",
        "    x =  x - alpha*Df\n",
        "    iters += 1\n",
        "  return x, iters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCCtfuRfno_x",
        "colab_type": "text"
      },
      "source": [
        "### **Newton's Method in $R^n$**\n",
        "\n",
        "Newton's method is based on the Taylor series (15.3)\n",
        "$$\n",
        "f(x) \\approx f(y) + \\nabla f(y)^T(x-y)+ \\frac{1}{2}(x-y)^T Hf(y)(x-y).\n",
        "$$\n",
        "Set $x= x^{(k+1)}, \\ y = x^{(k)}$ such that,\n",
        "$$\n",
        "\\frac{df(x^{(k+1)})}{d(\\Delta x)} = \\nabla f(x^{(k)}) + Hf(x^{(k)}) \\Delta x = 0.\n",
        "$$\n",
        "where $\\Delta x =  x^{(k+1)} - x^{(k)}$. Which gives us the increment for Newton's method for finding stationary points,\n",
        "$$\n",
        "\\Delta x =  - (Hf(x^{(k)}))^{-1} \\nabla f(x^{(k)}). \n",
        "$$\n",
        "With the iterative formula,\n",
        "$$\n",
        "x^{(k+1)} = x^{(k)} - (Hf(x^{(k)}))^{-1} \\nabla f(x^{(k)}).\n",
        "$$\n",
        "We implement algorithm 15.3 with $\\alpha = 1.0$ and stopping criteria\n",
        "$$\n",
        "\\Vert \\nabla f(x^{(k)}) \\Vert < TOL.\n",
        "$$\n",
        "Note that algorithm 15.3 says that $x = x - \\alpha dx$ and $dx = solve(Hf, -Df)$, but we need to change one of the signs to get the correct formula."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eByMiaHnnvXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def newtons_method(x0, grad, H, TOL= 0.001, alpha = 1.0):\n",
        "  x = x0\n",
        "  Df = grad(x)\n",
        "  iters = 0\n",
        "  while np.linalg.norm(Df) > TOL:\n",
        "    Df = grad(x)\n",
        "    Hf = H(x)\n",
        "    dx = (-1)*np.linalg.solve(Hf, Df)\n",
        "    x =  x + alpha*dx\n",
        "    iters += 1\n",
        "  return x, iters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_",
        "colab_type": "text"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd",
        "colab_type": "text"
      },
      "source": [
        "We are to verify the accuracy and convergence of our methods with respect to the exact solution. We test two different functions,\n",
        "$$\n",
        "f(x,y) = (1 - x^2 - y^2)^2, \\ \\ g(x,y,z) = x^2 + y^2 + z^2 + x + y\n",
        "$$\n",
        "which have minima, $f(x,y) = 0$ for $x^2+ y^2 = 1$ and $g(x,y,z) = -\\frac{1}{2}$ at $(-\\frac{1}{2}, -\\frac{1}{2}, 0)$. \n",
        "\n",
        "We start with gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvNkMubMBpay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "08b5be40-93d9-4910-e1c4-a3935831116d"
      },
      "source": [
        "f = lambda x: (1-x[0]*x[0] - x[1]*x[1]) * (1-x[0]*x[0] - x[1]*x[1])\n",
        "g = lambda x: x[0]*x[0] + x[1]*x[1] + x[2]*x[2] + x[0] + x[1]\n",
        "\n",
        "g_exact = np.array([-1/2, -1/2, 0])\n",
        "g_descent, iter_g_descent = gradient_descent_method(g, np.array([10.0,10.0,10.0]), TOL = 0.001, beta=0.5)\n",
        "f_descent, iter_f_descent = gradient_descent_method(f, np.array([2.0, 2.0]), TOL = 0.001, beta=0.5)\n",
        "print(\"GRADIENT DESCENT\")\n",
        "print(\"Stationary point f(x,y): \" + str(f_descent))\n",
        "print(\"x^2 +  y^2 = 1 check: \" + str(f_descent[0]*f_descent[0] + f_descent[1]*f_descent[1]))\n",
        "print(\"Iterations f: \" + str(iter_f_descent))\n",
        "print()\n",
        "print(\"Absolute Error g(x,y,z): \" + str(abs(g_descent - g_exact)))\n",
        "print(\"Iterations g: \" + str(iter_g_descent))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GRADIENT DESCENT\n",
            "Stationary point f(x,y): [0.70630603 0.70630603]\n",
            "x^2 +  y^2 = 1 check: 0.9977364030035543\n",
            "Iterations f: 63\n",
            "\n",
            "Absolute Error g(x,y,z): [0.00028484 0.00028484 0.00027127]\n",
            "Iterations g: 336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNrPj2FCDtxM",
        "colab_type": "text"
      },
      "source": [
        "The method converges to the minima within the tolererence.\n",
        "\n",
        "We continue with Newton's method for which we have to compute the gradient and Hessian for both functions.\n",
        "$$\n",
        "\\nabla f = (-4x+4x^3+ 4xy^2, \\ -4y+4yx^2+4y^3)^T,\n",
        "$$\n",
        "$$\n",
        "Hf = \\begin{pmatrix} -4 + 12x^2 + 4y^2 & 8xy \\\\ 8yx & -4 + 4x^2 + 12y^2 \\end{pmatrix},\n",
        "$$\n",
        "$$\n",
        "\\nabla g = (2x + 1, 2y + 1, 2z)^T,\n",
        "$$\n",
        "$$\n",
        "Hg = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 2& 0 \\\\ 0 & 0 & 2 \\end{pmatrix}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvbVZs-eGAhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "d1ac15aa-889b-43b1-dc15-3d0b4f3b377f"
      },
      "source": [
        "# grad and hessian for x^2 + y^2+ 10x (min at [-5,0])\n",
        "def fG(x):\n",
        "  g = np.array([[-4.0*x[0,0] + 4.0*(x[0,0]**3) + 4*x[0,0]*x[1,0]*x[1,0]], [-4.0*x[1,0] + 4.0*x[1,0]*x[0,0]*x[0,0] + 4.0*(x[1,0]**3)]])\n",
        "  return g\n",
        "def fH(x):\n",
        "  H = np.array([[-4.0 + 12.0*x[0,0]*x[0,0] + 4.0*x[1,0]*x[1,0], 8*x[0,0]*x[1,0]], [8*x[0,0]*x[1,0], -4.0 + 4.0*x[0,0]*x[0,0] + 12.0*x[1,0]*x[1,0]]])\n",
        "  return H\n",
        "\n",
        "def gG(x):\n",
        "  g = np.array([[2.0*x[0,0] + 1.0], [2.0*x[1,0] + 1], [2.0*x[2,0]]])\n",
        "  return g\n",
        "\n",
        "def gH(x):\n",
        "  H = np.array([[2.0,0.0,0.0], [0.0,2.0,0.0], [0.0,0.0,2.0]])\n",
        "  return H\n",
        "\n",
        "f_newton, iter_f_newton  = newtons_method(np.array([[2.0],[2.0]]), fG, fH)\n",
        "g_newton, iter_g_newton = newtons_method(np.array([[10.0],[10.0],[10.0]]), gG, gH)\n",
        "\n",
        "print(\"NEWTONS METHOD\")\n",
        "print(\"Stationary point f(x,y): \" + str(f_newton))\n",
        "print(\"x^2 +  y^2 = 1 check: \" + str(f_newton[0,0]*f_newton[0,0] + f_newton[1,0]*f_newton[1,0]))\n",
        "print(\"Iterations f: \" + str(iter_f_newton))\n",
        "print()\n",
        "print(\"Absolute Error g(x,y,z): \" + str(abs(g_newton.flatten() - g_exact)))\n",
        "print(\"Iterations g: \" + str(iter_g_newton))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NEWTONS METHOD\n",
            "Stationary point f(x,y): [[0.70710678]\n",
            " [0.70710678]]\n",
            "x^2 +  y^2 = 1 check: 1.0000000000019007\n",
            "Iterations f: 7\n",
            "\n",
            "Absolute Error g(x,y,z): [0. 0. 0.]\n",
            "Iterations g: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcGFH68ZJsEC",
        "colab_type": "text"
      },
      "source": [
        "Newton's method also converge to the minima of the two functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m",
        "colab_type": "text"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe",
        "colab_type": "text"
      },
      "source": [
        "The implementation of the two methods appears to be sound and they behave as expected. \n",
        "\n",
        "While it is expected that Newton's method is both more accurate and faster to converge than gradient descent, since it uses more information of the function. It was quite surprising just how much faster and accurate Newton's method was.  However, gradient descent uses an approximation of the gradient while Newton's has the exact Hessian, so the comparision is not entirely fair."
      ]
    }
  ]
}