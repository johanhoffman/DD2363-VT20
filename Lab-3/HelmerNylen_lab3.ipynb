{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DD2363 Lab 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363-VT20/blob/HelmerNylen/Lab-3/HelmerNylen_lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7",
        "colab_type": "text"
      },
      "source": [
        "# **Lab 3: Iterative Methods**\n",
        "**Helmer Nylén**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm",
        "colab_type": "text"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJipbXtnjrJZ",
        "colab_type": "text"
      },
      "source": [
        "We implement four iterative methods and verify their correctness: Jacobi iteration, Gauss-Seidel iteration, Newton's method, and the GMRES method. All of these were found to be correct, but Newton's method does have some problems with numerical accuracy in the tests used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3",
        "colab_type": "text"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo",
        "colab_type": "text"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is distributed under a certain license. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab_type": "code",
        "outputId": "a3122707-d724-4922-a332-104ab8d41c54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2019 Helmer Nylén (helmern@kth.se)\n",
        "\n",
        "# This file is part of the course DD2363 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version."
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh",
        "colab_type": "text"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa",
        "colab_type": "text"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load neccessary modules.\n",
        "import numpy as np\n",
        "from random import randint\n",
        "from numpy.polynomial import polynomial as P\n",
        "from functools import reduce"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6",
        "colab_type": "text"
      },
      "source": [
        "In this report we implement four iterative methods: Jacobi iteration, Gauss-Seidel iteration, a scalar version of Newton's method, and the GMRES method. These are based on algorithms described in the lecture notes combined with `numpy` objects and functionality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeFO9QMeUOAu",
        "colab_type": "text"
      },
      "source": [
        "# **Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx",
        "colab_type": "text"
      },
      "source": [
        "Chapter 7.3 describes fixed-point iteration using an algorithm known as _Richardson iteration_. We can use this as a basis for both Jacobi iteration and Gauss-Seidel iteration by preconditioning the iteration matrix with an approximate inverse. These methods are outlined below.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeJOAGxhru3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Algorithm 7.1, chapter 7.3\n",
        "def _richardson_iteration(A, b, alpha, tolerance = 1e-10):\n",
        "  assert A.shape[0] == b.shape[0] and 1 > alpha > 0 and tolerance > 0\n",
        "\n",
        "  x = np.zeros(A.shape[1])\n",
        "  r = None\n",
        "  while r is None or np.linalg.norm(r) > tolerance:\n",
        "    r = b - A @ x\n",
        "    x += alpha * r\n",
        "  return x\n",
        "\n",
        "def _left_precond(A, B, b, alpha, tolerance = 1e-10):\n",
        "  return _richardson_iteration(B @ A, B @ b, alpha, tolerance)\n",
        "\n",
        "def _right_precond(A, B, b, alpha, tolerance = 1e-10):\n",
        "  x = _richardson_iteration(A @ B, b, alpha, tolerance)\n",
        "  return B @ x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3amdK44wZUE",
        "colab_type": "text"
      },
      "source": [
        "According to the lecture notes page 133, Jacobi iteration is equivalent to left preconditioned Richardson iteration using $B = (\\alpha D)^{-1}$, where $D$ is a diagonal matrix with the same elements as the diagonal of $A$. Given our above functions this yields a simple implementation of Jacobi iteration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMkghuxpxH7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jacobi(A, b, alpha = 0.05, tolerance = 1e-10, richardson = True):\n",
        "  Dvec = np.diag(A)\n",
        "  assert not (Dvec == 0).any()\n",
        "  Dvec = Dvec * alpha\n",
        "  Dinv = np.diag(1 / Dvec)\n",
        "  return _left_precond(A, Dinv, b, alpha, tolerance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldx9TT1-8Y1w",
        "colab_type": "text"
      },
      "source": [
        "We can only be certain that the Jacobi iteration will converge when the spectral radius $\\rho$ of the iteration matrix $M_j$ is less than one, i.e. $$\\rho(M_j) = \\rho(I - D^{-1} A) < 1.$$ We ensure that this is the case before testing our random matrix $A$.\n",
        "\n",
        "We also know that \"convergence of the Jacobi iteration depends on how close $A$ is to a diagonal matrix\" (lecture notes page 133), and as such make sure to emphasize the diagonal when generating $A$.\n",
        "\n",
        "We verify convergence by asserting that both the residual and that the distance to the manufactured solution is small."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brp-CVl5ZLzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _spectral_radius(A):\n",
        "  return max(map(abs, np.linalg.eigvals(A)))\n",
        "\n",
        "def test_jacobi(richardson = True):\n",
        "  n = randint(1, 10)\n",
        "  A = None\n",
        "  while A is None or _spectral_radius(np.eye(n) - np.diag(1 / np.diag(A)) @ A) >= 1:\n",
        "    A = np.random.rand(n, n)\n",
        "    Adiag = 10 * np.random.rand(n) + 1\n",
        "    A = A + np.diag(Adiag)\n",
        "    # make A symmetric\n",
        "    for i in range(n):\n",
        "      for j in range(i):\n",
        "        A[i, j] = A[j, i]\n",
        "\n",
        "  y = np.random.rand(n)\n",
        "  b = A @ y\n",
        "  x = jacobi(A, b, richardson = richardson)\n",
        "\n",
        "  return np.isclose(0, np.linalg.norm(A @ x - b), atol=1e-10) and \\\n",
        "         np.isclose(0, np.linalg.norm(x - y), atol=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp3CthUgr0i1",
        "colab_type": "text"
      },
      "source": [
        "Similarly to Jacobi iteration, Gauss-Seidel iteration can be defined via Richardson iteration left-preconditioned with $B = (\\alpha L)^{-1}$ where $L$ is the lower triangular matrix with its nonzero elements $l_{ij} = a_{ij}$, $i \\geq j$. Inverting $L$ can be done via forward substitution as defined below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bPe9viD8Apy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _forward_substitution(L, b):\n",
        "  n = L.shape[1]\n",
        "  x = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    s = x.dot(L[i, :])\n",
        "    x[i] = (b[i] - s) / L[i, i]\n",
        "  return x\n",
        "\n",
        "def _inv(L):\n",
        "  n = L.shape[0]\n",
        "  eye = np.eye(n)\n",
        "  inv = np.zeros(L.shape)\n",
        "  for i in range(n):\n",
        "    inv[:, i] = _forward_substitution(L, eye[:, i])\n",
        "  return inv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sLHiHIP0JJS",
        "colab_type": "text"
      },
      "source": [
        "The definition of Gauss-Seidel iteration is then trivial given the already defined functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdMj2W1tr08X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gauss_seidel(A, b, alpha = 0.05):\n",
        "  L = np.tril(A)\n",
        "  Linv = _inv(alpha * L)\n",
        "  return _left_precond(A, Linv, b, alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzEnyf9AZXXP",
        "colab_type": "text"
      },
      "source": [
        "Again, we can only be certain of convergence when $$\\rho(M_{GS}) = \\rho(I-L^{-1}A) < 1,$$ and thus use matrices $A$ which satisfy that condition. We generate these matrices in a similar manner to the Jacobi tests and verify the same quantities for convergence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOjmcTFFZX2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_gs():\n",
        "  n = randint(1, 10)\n",
        "  A = None\n",
        "  while A is None or _spectral_radius(np.eye(n) - np.linalg.inv(np.tril(A)) @ A) >= 1:\n",
        "    A = np.random.rand(n, n)\n",
        "    Adiag = 10 * np.random.rand(n) + 1\n",
        "    A = A + np.diag(Adiag)\n",
        "    # make A symmetric\n",
        "    for i in range(n):\n",
        "      for j in range(i):\n",
        "        A[i, j] = A[j, i]\n",
        "\n",
        "  y = np.random.rand(n)\n",
        "  b = A @ y\n",
        "  x = gauss_seidel(A, b)\n",
        "  \n",
        "  return np.isclose(0, np.linalg.norm(A @ x - b), atol=1e-10) and \\\n",
        "         np.isclose(0, np.linalg.norm(x - y), atol=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uabg_Wyr8cz",
        "colab_type": "text"
      },
      "source": [
        "For Newton's method, we begin by defining a numerical derivative using the central difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwccrK13UmoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _derivative(f, x, h = 1e-12):\n",
        "  return (f(x + h) - f(x - h)) / (2 * h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkZ40cr82Z12",
        "colab_type": "text"
      },
      "source": [
        "We then use this in our implementation of algorithm 8.2, chapter 8.3 of the lecture notes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "878gNQSdr8AP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Algorithm 8.2, chapter 8.3\n",
        "def newton_scalar(f, x0 = 0, tolerance = 1e-10):\n",
        "  assert hasattr(f, '__call__')\n",
        "  x = x0\n",
        "  while abs(f(x)) > tolerance:\n",
        "    df = _derivative(f, x)\n",
        "    x -= f(x) / df\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSDPrNWFZlm1",
        "colab_type": "text"
      },
      "source": [
        "To test this method we create a polynomial $f$ of a certain degree $n$ with known zeroes $\\{z_i\\}$. Given an arbitrary $x_0$ in the domain of $f$ we can then ensure that the $x$ the algorithm yields satisfies $$f(x) \\approx 0$$ and $$|x-z_i| \\approx 0$$ for some $z_i$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab-sSWLKZm2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_newton():\n",
        "  n = randint(1, 5)\n",
        "  zeros = np.random.rand(n) * 20 - 10\n",
        "  polynomials = [(-zeros[i], 1) for i in range(n)]\n",
        "  pol = reduce(P.polymul, polynomials)\n",
        "  f = lambda x: P.polyval(x, pol)\n",
        "  try:\n",
        "    for _ in range(n):\n",
        "      x = newton_scalar(f, x0 = np.random.rand() * 20 - 10)\n",
        "      assert np.isclose(0, abs(f(x)), atol=1e-10)               # x is a zero of f\n",
        "      assert np.isclose(0, min(map(abs, x - zeros)), atol=1e-4) # x is close to a listed zero\n",
        "    return True\n",
        "  except AssertionError:\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C5wpuh6BlkH",
        "colab_type": "text"
      },
      "source": [
        "For the GMRES method we implement algorithms 7.2 and 7.3, chapter 7.4, for GMRES and Arnoldi iteration, respectively. The specifics of the algorithms are modified slightly due to minor errors in the lecture notes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF2-10yQ0GEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _standard_basis(k, i=0):\n",
        "  return (np.array(range(k)) == i) * 1\n",
        "\n",
        "def gmres(A, b, tolerance = 1e-10):\n",
        "  k = 1\n",
        "  r = None\n",
        "  while r is None or np.linalg.norm(r) / np.linalg.norm(b) > tolerance:\n",
        "    Q, H = arnoldi(A, b, k)\n",
        "    y = np.linalg.lstsq(H, np.linalg.norm(b) * _standard_basis(k + 1))[0]\n",
        "    r = H @ y\n",
        "    r = np.linalg.norm(b) * _standard_basis(k + 1) - r\n",
        "    k += 1\n",
        "  return Q[:, 0:k-1] @ y\n",
        "\n",
        "def arnoldi(A, b, k):\n",
        "  Q = np.zeros((b.shape[0], k+1))\n",
        "  H = np.zeros((k + 1, k))\n",
        "  Q[:, 0] = b / np.linalg.norm(b)\n",
        "  for j in range(k):\n",
        "    v = A @ Q[:, j]\n",
        "    for i in range(j + 1):\n",
        "      H[i, j] = Q[:, i].dot(v)\n",
        "      v = v - H[i, j] * Q[:, i]\n",
        "    H[j + 1, j] = np.linalg.norm(v)\n",
        "    Q[:, j + 1] = v / H[j + 1, j]\n",
        "  return Q, H"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8iwYTzSnCai",
        "colab_type": "text"
      },
      "source": [
        "The GMRES method is not as dependent on the properties of $A$ as the Jacobi or Gauss-Seidel iterations, and thus we need not ensure anything in particular about our randomized $A$ before testing it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSODdcLnC3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_gmres():\n",
        "  n = randint(1, 10)\n",
        "  A = np.random.rand(n, n)\n",
        "\n",
        "  y = np.random.rand(n)\n",
        "  b = A @ y\n",
        "  x = gmres(A, b)\n",
        "  \n",
        "  return np.isclose(0, np.linalg.norm(A @ x - b), atol=1e-10) and \\\n",
        "         np.isclose(0, np.linalg.norm(x - y), atol=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_",
        "colab_type": "text"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmRaodmdZyTn",
        "colab_type": "code",
        "outputId": "af9cd63a-75d4-4303-d292-5a265114569d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "total_tests = 1000\n",
        "jacobi_sum = 0\n",
        "gs_sum = 0\n",
        "newton_sum = 0\n",
        "gmres_sum = 0\n",
        "for i in range(total_tests):\n",
        "  #print(str(i).rjust(3), end=\" \\n\"[(i+1) % 100 == 0])\n",
        "  jacobi_sum += test_jacobi()\n",
        "  gs_sum += test_gs()\n",
        "  newton_sum += test_newton()\n",
        "  gmres_sum += test_gmres()\n",
        "\n",
        "print(f\"{jacobi_sum} of {total_tests} jacobi tests passed\")\n",
        "print(f\"{gs_sum} of {total_tests} gauss-seidel tests passed\")\n",
        "print(f\"{newton_sum} of {total_tests} newton tests passed\")\n",
        "print(f\"{gmres_sum} of {total_tests} gmres tests passed\")"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/numpy/polynomial/polynomial.py:733: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  c0 = c[-1] + x*0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000 of 1000 jacobi tests passed\n",
            "1000 of 1000 gauss-seidel tests passed\n",
            "991 of 1000 newton tests passed\n",
            "1000 of 1000 gmres tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd",
        "colab_type": "text"
      },
      "source": [
        "We note that all Jacobi, Gauss-Seidel and GMRES tests tend to pass while a few of the Newton tests fail. We also note a number of error messages from numpy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m",
        "colab_type": "text"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe",
        "colab_type": "text"
      },
      "source": [
        "As all tests for $Ax=b$ solvers pass we assume these to be correct.\n",
        "\n",
        "The motivation for using polynomials to test the implementation of Newton's method was that all analytic scalar functions can be arbitrarily well approximated with a polynomial using Taylor expansion (Taylor's theorem). However, during testing with zeros of multiplicity greater than one the method failed much more often. This is likely due to $$f'(x) = f(x) = 0$$ at zeroes of $f$, which hinders convergence. The fact that the method still fails occasionally despite that all zeroes have a multiplicity of one can likely be attributed to numerical errors, as we are dealing with very small numbers (close to the machine epsilon listed below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeYpHmNK7OT4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0a8647e9-0260-4868-ed60-1849b56d05df"
      },
      "source": [
        "import sys; sys.float_info.epsilon"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.220446049250313e-16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    }
  ]
}