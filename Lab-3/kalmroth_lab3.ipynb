{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kalmroth_lab3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7",
        "colab_type": "text"
      },
      "source": [
        "# **Lab 3: Iterative methods**\n",
        "**Kristoffer Almroth**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm",
        "colab_type": "text"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL",
        "colab_type": "text"
      },
      "source": [
        "Third lab in the course DD2363 Methods in Scientific Computing. This lab is about iterative methods for solving the equation Ax=b and f(x)=0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh",
        "colab_type": "text"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa",
        "colab_type": "text"
      },
      "source": [
        "Dependencies needed for running the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import sympy as sp\n",
        "from sympy import *\n",
        "import random\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "import unittest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6",
        "colab_type": "text"
      },
      "source": [
        "One important function of matrices is to be able to solve the equation Ax=b. This equation can be solved by a variety of different methods, imcluding direct methods and iterative methods. In this lab three iterative methods will be implemented: Jacobi iteration, Gauss-Seidel iteration and GMRES. Additionally, Newton's method for scalar nonlinear equations will be implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeFO9QMeUOAu",
        "colab_type": "text"
      },
      "source": [
        "# **Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx",
        "colab_type": "text"
      },
      "source": [
        "This section is based on the theory from the fourth and fifth lecture and the lecture notes. Some of the algorithms are based on the pseudo code described in the lecture notes. All code is tested using various different test cases, including the test cases given in the lab requirements and some additional tests. \n",
        "\n",
        "Since floating point precision were used in the tests, there could be rounding errors.  The test methods of the library [numpy](https://docs.scipy.org/doc/numpy/reference/routines.testing.html) is used to test equality to a certain precision.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OQvPD5k407N",
        "colab_type": "text"
      },
      "source": [
        "## Iterative methods for $Ax=b$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCxc1wXop_gC",
        "colab_type": "text"
      },
      "source": [
        "### Jacobi iteration\n",
        "\n",
        "Jacobi iteration works through matrix splitting, where $A = A_1 + A_2$. $A_1$ is the diagonal elements of $A$ while $A_2 = A - A_1$.\n",
        "\n",
        "$x^{(k+1)} = (I-D^{-1}A)x^{(k)} + D^{-1}b$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jgpUUhqTcmnj",
        "colab": {}
      },
      "source": [
        "def jacobi(A, b, TOL):\n",
        "\n",
        "  n = A.shape[0]\n",
        "  I = np.identity(n)\n",
        "  D = np.diag(np.diag(A))\n",
        "  D_inv = np.linalg.inv(D)\n",
        "  c = D_inv.dot(b)\n",
        "  x = c\n",
        "\n",
        "  while np.linalg.norm(A.dot(x) - b) > TOL:\n",
        "    x = (I-np.matmul(D_inv,A)).dot(x) + c\n",
        "\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bZiDf6IGjagD"
      },
      "source": [
        "### Gauss-Seidel iteration\n",
        "\n",
        "Gauss-Seidel iteration also works through matrix splitting, but instead of a diagonal matrix, $A_1 = L$ where L is a lower triangluar matrix.\n",
        "\n",
        "$x^{(k+1)} = (I-L^{-1}A)x^{(k)} + L^{-1}b$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aAsmTy8SjxOM",
        "colab": {}
      },
      "source": [
        "def GaussSeidel(A, b, TOL):\n",
        "\n",
        "  n = A.shape[0]\n",
        "  I = np.identity(n)\n",
        "  L = np.tril(A)\n",
        "  L_inv = np.linalg.inv(L)\n",
        "  c = L_inv.dot(b)\n",
        "  x = c\n",
        "\n",
        "  while np.linalg.norm(A.dot(x) - b) > TOL:\n",
        "    x = (I-np.matmul(L_inv,A)).dot(x) + c\n",
        "\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i9ruDkpqGej",
        "colab_type": "text"
      },
      "source": [
        "### Test cases\n",
        "\n",
        "A requirement for convergence is that $||I-\\alpha A|| < 1$. Randomly generated test data does not usually fill this requirement, so a precondition is used. $BAx=Bb$ where B is close to the inverse of A makes the randomly generated matrix always converge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "772osfO6vs-H",
        "colab_type": "code",
        "outputId": "7ce12319-3243-43c8-8e75-ee00c29f0ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "class Test(unittest.TestCase):\n",
        "\n",
        "  def testRandom(self):\n",
        "    for i in range(0,1000):\n",
        "      row = randrange(30) + 10\n",
        "      A = np.random.rand(row,row)\n",
        "      x = np.random.rand(row)\n",
        "      b = A.dot(x)\n",
        "\n",
        "      # Preconditioning for convergence\n",
        "      B = np.linalg.inv(A)\n",
        "      B = B * 0.8\n",
        "      A = np.matmul(B,A)\n",
        "      b = B.dot(b)\n",
        "\n",
        "      x2 = jacobi(A,b,1e-7)\n",
        "      x3 = GaussSeidel(A,b,1e-7)\n",
        "\n",
        "      # x = A^(-1)b\n",
        "      np.testing.assert_allclose(x2, x, rtol=1e-5, atol=0)\n",
        "      np.testing.assert_allclose(x3, x, rtol=1e-5, atol=0)\n",
        "\n",
        "      # ||Ax-b|| = 0\n",
        "      np.testing.assert_almost_equal(np.linalg.norm(A.dot(x2)-b), 0, decimal=5)\n",
        "      np.testing.assert_almost_equal(np.linalg.norm(A.dot(x3)-b), 0, decimal=5)\n",
        "\n",
        "      # ||x-x2|| = 0\n",
        "      np.testing.assert_almost_equal(np.linalg.norm(x-x2), 0, decimal=5)\n",
        "      np.testing.assert_almost_equal(np.linalg.norm(x-x3), 0, decimal=5)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 1.219s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8MDHKJpCkq5Z"
      },
      "source": [
        "## Newton's method for scalar nonlinear equation\n",
        "\n",
        "Newton's method is based on the first order derivative of the nonlinear function.\n",
        "\n",
        "$x^{(k+1)} = x^{(k)} - f^{'}(x^{(k)})^{-1} f(x^{(k)})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q3mc0TbmtFlW",
        "colab": {}
      },
      "source": [
        "def NewtonsMethod(f, y, TOL):\n",
        "\n",
        "  # Using sympy to calculate f'\n",
        "  x = Symbol('x')\n",
        "  fprime = f.diff(x)\n",
        "  f = lambdify(x, f)\n",
        "  fprime = lambdify(x, fprime)\n",
        "\n",
        "  while np.abs(f(y)) > TOL:\n",
        "    y -= f(y)/fprime(y)\n",
        "\n",
        "  return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ-USJk3wuCg",
        "colab_type": "code",
        "outputId": "a8f236e7-a30e-46ad-cff1-1eeea27d2bf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "class Test(unittest.TestCase):\n",
        "\n",
        "  def testRandom(self):\n",
        "    for i in range(0,1000):\n",
        "\n",
        "      # Generating a random function using sympy\n",
        "      exponents = [random.randint(1,10) for i in range(3)]\n",
        "      x = Symbol('x')\n",
        "      f = x**exponents[0] + x**exponents[1] + x**exponents[2] - random.randint(0,100)\n",
        "\n",
        "      y = NewtonsMethod(f, 1, 1e-7)\n",
        "      f = lambdify(x, f)\n",
        "\n",
        "      # |f(x)|\n",
        "      np.testing.assert_almost_equal(np.abs(f(y)), 0, decimal=5)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 4.189s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYX5u9k7xCkA",
        "colab_type": "text"
      },
      "source": [
        "## GMRES method\n",
        "\n",
        "Generalised minimal residual, where we want to calculate $\\displaystyle \\min_{y \\in \\mathbb{R}^k} ||b-AQ_ky||$ where $Q_k$ is an orthonormal basis for $K_k$. Through Arnoldi iteration we get $\\displaystyle \\min_{y \\in \\mathbb{R}^k} || \\space ||b||e_1-H_ky||$.\n",
        "\n",
        "We get the approximation through $x^{(k)} = Q_ky$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9Ua1OMgxNLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def arnoldiIteration(A, b, k):\n",
        "  n = A.shape[0]\n",
        "  Q = np.zeros(shape=(n,k+1))\n",
        "  H = np.zeros(shape=(k+1,k))\n",
        "  v = np.zeros(n)\n",
        "\n",
        "  Q[:,0] = b / np.linalg.norm(b)\n",
        "  for j in range(0,k):\n",
        "    v = A.dot(Q[:,j])\n",
        "    for i in range(0, j):\n",
        "      H[i,j] = Q[:,i].dot(v)\n",
        "      v = v - H[i,j] * Q[:,i]\n",
        "    H[j+1,j] = np.linalg.norm(v)\n",
        "    Q[:,j+1] = v / H[j+1,j]\n",
        "  \n",
        "  return Q, H\n",
        "\n",
        "def gmres(A, b, TOL):\n",
        "\n",
        "  n = A.shape[0]\n",
        "  Q = np.zeros(shape=(n,n))\n",
        "  x = np.zeros(n)\n",
        "  k = 1\n",
        "  r = b * 2\n",
        "\n",
        "  Q[:,0] = b / np.linalg.norm(b)\n",
        "\n",
        "  while np.linalg.norm(r) / np.linalg.norm(b) > TOL:\n",
        "    (Q, H) = arnoldiIteration(A, b, k)\n",
        "\n",
        "    e_k = np.zeros(k+1)\n",
        "    e_k[0] = np.linalg.norm(b)\n",
        "    y = np.linalg.lstsq(H, e_k)[0]\n",
        "\n",
        "    r = H.dot(y)\n",
        "    r = -r\n",
        "    r[0] += np.linalg.norm(b)\n",
        "\n",
        "    k = k+1  \n",
        "\n",
        "  # Q matrix is of wrong size, downscale it\n",
        "  Q = np.delete(Q, k-1, 1)\n",
        "\n",
        "  x = (Q).dot(y)\n",
        "\n",
        "  return x;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsOFTmYZzIjw",
        "colab_type": "code",
        "outputId": "436fc01f-7dc4-4e2f-f963-0197ae494901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "class Test(unittest.TestCase):\n",
        "\n",
        "  def testRandom(self):\n",
        "    for i in range(0,2):\n",
        "      row = randrange(10) + 10\n",
        "      A = np.random.rand(row,row)\n",
        "      x = np.random.rand(row)\n",
        "      b = A.dot(x)\n",
        "      x2 = gmres(A,b,1e-6)\n",
        "\n",
        "      # x = A^(-1)b\n",
        "      np.testing.assert_allclose(x2, x, rtol=1e-2, atol=0)\n",
        "\n",
        "      # ||Ax-b|| = 0\n",
        "      np.testing.assert_almost_equal(np.linalg.norm(A.dot(x2)-b), 0, decimal=2)\n",
        "\n",
        "      # ||x-x2|| = 0\n",
        "      np.testing.assert_almost_equal(np.linalg.norm(x-x2), 0, decimal=2)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 42.145s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_",
        "colab_type": "text"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd",
        "colab_type": "text"
      },
      "source": [
        "The iterative methods Jacobi iteration, Gauss-Seidel iteration and Newton's method were accurate to more than five decimals when the methods were called with the precision $1e-7$.\n",
        "\n",
        "GMRES were accurate up to the second decimal when called with the precision $1e-6$. Each test took approximataly six seconds, which is slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m",
        "colab_type": "text"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe",
        "colab_type": "text"
      },
      "source": [
        "I find it weird that the dimensions of Q and y in the GMRES functions did not align, which was solved by removing the outer column in the Q matrix. Based on the specification of numpy least square algorithm, y should be of size k, which does not match with Q that is of size $n \\space x \\space k+1 $. The solution $x$ still seems to converge to the correct value, so this potential error is acceptable."
      ]
    }
  ]
}