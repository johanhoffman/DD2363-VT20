{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of patrik-svensson_lab_matrix_algorithms.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7",
        "colab_type": "text"
      },
      "source": [
        "# **Lab3: Iterative Methods**\n",
        "**Patrik Svensson**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm",
        "colab_type": "text"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixOh5-9HXB0N",
        "colab_type": "text"
      },
      "source": [
        "In this lab report, the concept of iterative methods of solving linear equations, and also non-linear functions are explored. It is possible to solve linear equations by using direct method, but it is not always the most suitable from a computional and performance view, therefore iterative methods are presented here as a substitute. The report investigates four different algorithms for solving linear and non-linear equations, *Jacobi iteration*, *Gauss-Seidel method*, and *Newton's method for scalar nonlinear equation*. The result of the report is four implemented functions in Python, together with unit tests that confirms the correctness of the implementation of the algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh",
        "colab_type": "text"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa",
        "colab_type": "text"
      },
      "source": [
        "To set up the environment, run the two following lines of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import unittest\n",
        "import math as math\n",
        "import scipy.misc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6",
        "colab_type": "text"
      },
      "source": [
        "In the previous lab we solved linear equations on the classical form $Ax = b$ with direct methods, in other words, by doing factorization. But when the matrices get to big, on a million dimension level, the method by using factorization becomes unpractical. To mitigate this issue, we are introducing a new approach called *iterative methods*. Instead of finding an exact solution to a linear equation, we are satisfied with an aproximate solution. We achive this by iterating through an algorithm that is getting closer and closer to the solution of the linear eqation, and when it is close enough to the correct solution, the algorithm stops and return the solution (with a minor error). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeFO9QMeUOAu",
        "colab_type": "text"
      },
      "source": [
        "# **Methods**\n",
        "In this chapter, I will present how the implementation of the functions was conducted. The study was conducted in the following way.\n",
        "\n",
        "1.   Literature research\n",
        "2.   Implementation\n",
        "3.   Testing\n",
        "\n",
        "In the sections below, I have provided a reference to where the algorithms were founded, or how it was deduced, followed with a code implementation in Python, and lastly unit test for the assurance of the accuracy of the implementations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J8UcBbqYQLq",
        "colab_type": "text"
      },
      "source": [
        "## Jacobi iteration\n",
        "When deciding to solve a linear equation system with a Jacobi iteration, we start with the following expansion of the classic linear equation that we want to solve. \n",
        "\n",
        "$Ax = b \\leftrightarrow\n",
        "\\\\(D + L + U)x = b \\leftrightarrow\n",
        "\\\\Dx = b - (L + U)x \\leftrightarrow\n",
        "\\\\x = D^{-1}(b - (L + U)x)$\n",
        "\n",
        "Where $D$ is the diagonal, $L$ is the lower triangular matrix, $U$ is the upper triangular matrix of $A$. The Jacobi iteration is a fixed-point iteration of the expansion above. When performing a Jacobi iteration, the algorithm is the following.\n",
        "\n",
        "$x_0 = initial \\: vector\\\\\n",
        "x_{k+1} = D^{-1}(b - (L + U)x_k) \\: \\textbf{for} \\: k = 0,1,2,3,...$\n",
        "\n",
        "This according to (2.40) in the book *Numerical Analysis* that is a compiled version of Timonthy Sauer's book with the same made for KTH and published by Pearson. The code for the algorithm below is inpired by the matlab code in example 2.25 from the same book. The algorithm will continue to iterated until the residual is less than *TOL*. Below is an implementation of the algorithm in Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcGkjaOAYRFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jacobi_iteration(A, b):\n",
        "  n = b.shape[0]\n",
        "  diagonal = np.diag(np.diag(A))\n",
        "  diagonal_inverted = np.linalg.inv(diagonal)\n",
        "  r = A - np.diag(np.diag(A))\n",
        "  x = np.zeros(n)\n",
        "  TOL = 0.01\n",
        "  residual = np.full(n, np.Infinity)\n",
        "  might_diverge = False\n",
        "  MAX_ITERATIONS = 100\n",
        "\n",
        "  # Strictly diagonally dominant check\n",
        "  diagonal_array = np.diag(A)\n",
        "  for i in range(0, n):\n",
        "    for j in range(0, n):\n",
        "      if(A[i,j] > diagonal_array[i]):\n",
        "        might_diverge = True\n",
        "\n",
        "  if (might_diverge):\n",
        "    for i in range(0, MAX_ITERATIONS):\n",
        "      if not (np.linalg.norm(residual) > TOL):\n",
        "        break\n",
        "      residual = np.dot(A, x) - b \n",
        "      x = np.dot(diagonal_inverted,b-np.dot(r,x))\n",
        "  else:\n",
        "    while np.linalg.norm(residual) > TOL:\n",
        "      residual = np.dot(A, x) - b \n",
        "      x = np.dot(diagonal_inverted,b-np.dot(r,x))\n",
        "\n",
        "  if np.linalg.norm(residual) > TOL:\n",
        "    return None\n",
        "\n",
        "  return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbLkT4hSbami",
        "colab_type": "text"
      },
      "source": [
        "In the code below there is unit tests that are used to assure the correctness of the implemented Jacobi iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY2fiabXU7J6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TestJacobiIteration(unittest.TestCase):\n",
        "  def test_residual_Ax_b_one(self):\n",
        "    # Arrange\n",
        "    A = np.array([[3,1],\n",
        "              [1,2]])\n",
        "    expected_b = np.array([5,5])\n",
        "\n",
        "    # Act\n",
        "    x = jacobi_iteration(A, expected_b)\n",
        "    b = np.dot(A, x)\n",
        "\n",
        "    # Assert\n",
        "    self.assertAlmostEqual(0, np.linalg.norm(expected_b - b), 2)\n",
        "\n",
        "  def test_residual_x_y_one(self):\n",
        "    # Arrange\n",
        "    A = np.array([[3,1],\n",
        "              [1,2]])\n",
        "    b = np.array([5,5])\n",
        "    expected_y = np.array([1,2])\n",
        "\n",
        "    # Act\n",
        "    x = jacobi_iteration(A, b)\n",
        "\n",
        "    # Assert\n",
        "    self.assertAlmostEqual(0, np.linalg.norm(x - expected_y), 2)\n",
        "\n",
        "  def test_singular_matrix(self):\n",
        "    # Arrange\n",
        "    A = np.array([[1,2],\n",
        "              [1,2]])\n",
        "    b = np.array([5,5])\n",
        "    # Act\n",
        "    x = jacobi_iteration(A, b)\n",
        "\n",
        "    # Assert\n",
        "    self.assertEqual(None, x)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Help from user Pierre S. in the stack overflow thread to give the main arguments: \n",
        "    # https://stackoverflow.com/questions/49952317/python3-for-unit-test-attributeerror-module-main-has-no-attribute-kerne \n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p-Q9erwYVfB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Gauss-Seidel method\n",
        "The Gauss-Seidel method aims to approximately solve and linear equation on the form $Ax = b$, and it is very similar to the Jacobi iteration, and can be seen as a tweak of the Jacobi iteration. The difference lies in the idea that as we are calculating the elements of $x$ vector sequentialy, we could use the already calculated elements in the calculation of the remaining elements as we calculate the remaining elements of the $x$ vector. From the standard form of the linear equation, we can expand it the following way.\n",
        "\n",
        "$Ax = b\n",
        "\\\\ (L + D + U)x = b\n",
        "\\\\ (L + D)x = -Ux + b$\n",
        "\n",
        "Which can be used as a fixed-point iteration:\n",
        "\n",
        "$(L + D)x_{k+1} = -Ux_k + b\n",
        "\\\\x_{k+1} = D^{-1}(b - Ux_k - Lx_{k+1})$\n",
        "\n",
        "The Gauss-Seidel method can be expressed as an algorithm on the following form, which is taken from the book *Numerical Analysis* that is a compiled version of Timonthy Sauer's book with the same made for KTH and published by Pearson. The algorithm will continue to iterated until the residual is less than *TOL*.\n",
        "\n",
        "$x_0 = initial \\: vector\\\\\n",
        "x_{k+1} = D^{-1}(b - Ux_k - Lx_{k+1}) \\: \\textbf{for} \\: k = 0,1,2,3,...$\n",
        "\n",
        "The code below is the implementation of Gauss-Seidel in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mI8OBOCY3iA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gauss_seidel_method(A, b):\n",
        "  n = b.shape[0]\n",
        "  diagonal = np.diag(np.diag(A))\n",
        "  diagonal_inverted = np.linalg.inv(diagonal)\n",
        "  L = np.tril(A) - diagonal\n",
        "  U = np.triu(A) - diagonal\n",
        "  x = np.zeros(n)\n",
        "  TOL = 0.0001\n",
        "  residual = np.full(n, np.Infinity)\n",
        "  might_diverge = False\n",
        "  MAX_ITERATIONS = 100\n",
        "\n",
        "  # Strictly diagonally dominant check\n",
        "  diagonal_array = np.diag(A)\n",
        "  for i in range(0, n):\n",
        "    for j in range(0, n):\n",
        "      if(A[i,j] > diagonal_array[i]):\n",
        "        might_diverge = True\n",
        "\n",
        "  if (might_diverge):\n",
        "    for i in range(0, MAX_ITERATIONS):\n",
        "      if not (np.linalg.norm(residual) > TOL):\n",
        "        break\n",
        "      for i in range(0, n):\n",
        "        x[i] = (b[i] - np.dot(U[i],x) - np.dot(L[i],x)) * diagonal_inverted[i][i]\n",
        "      residual = np.dot(A, x) - b \n",
        "  else:\n",
        "    while np.linalg.norm(residual) > TOL:\n",
        "      residual = np.dot(A, x) - b \n",
        "      for i in range(0, n):\n",
        "        x[i] = (b[i] - np.dot(U[i],x) - np.dot(L[i],x)) * diagonal_inverted[i][i]\n",
        "\n",
        "  if np.linalg.norm(residual) > TOL:\n",
        "    return None\n",
        "\n",
        "  return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ootEFi15ciSF",
        "colab_type": "text"
      },
      "source": [
        "In the code below there is unit tests that are used to assure the correctness of the implemented Gauss-Seidel method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVuJ11EIcisn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TestGaussSeidelMethod(unittest.TestCase):\n",
        "  def test_residual_Ax_b_one(self):\n",
        "    # Arrange\n",
        "    A = np.array([[3,1],\n",
        "                  [1,2]])\n",
        "    expected_b = np.array([5,5])\n",
        "\n",
        "    # Act\n",
        "    x = gauss_seidel_method(A, expected_b)\n",
        "    b = np.dot(A, x)\n",
        "\n",
        "    # Assert\n",
        "    self.assertAlmostEqual(0, np.linalg.norm(expected_b - b), 2)\n",
        "\n",
        "  def test_residual_x_y_one(self):\n",
        "    # Arrange\n",
        "    A = np.array([[3,1],\n",
        "              [1,2]])\n",
        "    b = np.array([5,5])\n",
        "    expected_y = np.array([1,2])\n",
        "\n",
        "    # Act\n",
        "    x = gauss_seidel_method(A, b)\n",
        "\n",
        "    # Assert\n",
        "    self.assertAlmostEqual(0, np.linalg.norm(x - expected_y), 2)\n",
        "\n",
        "  def test_singular_matrix(self):\n",
        "    # Arrange\n",
        "    A = np.array([[1,2],\n",
        "                  [1,2]])\n",
        "    b = np.array([10,5])\n",
        "    # Act\n",
        "    x = gauss_seidel_method(A, b)\n",
        "\n",
        "    # Assert\n",
        "    self.assertEqual(None, x)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Help from user Pierre S. in the stack overflow thread to give the main arguments: \n",
        "    # https://stackoverflow.com/questions/49952317/python3-for-unit-test-attributeerror-module-main-has-no-attribute-kerne \n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "New1c67iY4E9",
        "colab_type": "text"
      },
      "source": [
        "## Newton's method for scalar nonlinear equation\n",
        "Compared to the previous investigated algorithms, Jacobi iteration, and Gauss-Seidel method, they work well when we want to find a solution to a linear quation, but they do not work when we are trying to find the root to a non-linear function. Newton's method is an iterative algorithm, like the previously presented algorithms. The idea behind the algorithm is to getting closer and closer to the root by running the algorithm several time, in other words, iterating.\n",
        "\n",
        "In the algorithm, you start by picking a arbitrary point $x_0$ in the domain of a $f(x)$ that you want to find to root in, which then you use to get the tangent-line of $f(x)$ in $x_0$. The value of x where the tangent-line cuts the x-axis is the new x value $x_1$ which is closer to the root if it converges. This procedure is repeated until the x value is sufficiently close to the root value. This process can be expressed on the following form:\n",
        "\n",
        "$x_0 = initial \\: guess\n",
        "\\\\ x_{i+1} = x_i - \\frac{f(x_i)}{f'(x_i)} \\: \\textbf{for} \\: i = 0,1,2,...$\n",
        "\n",
        "Which is provided in the book *Numerical Analysis* that is a compiled version of Timonthy Sauer's book with the same made for KTH and published by Pearson. The intial guess is a quite difficult issue, therefore I'm setting it to one.\n",
        "\n",
        "The Python implementation below is based on the algorithm 8.1 from the course lecture notes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKvnfYPbY5A7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def newtons_method_scalar(f):\n",
        "  x = 1\n",
        "  TOL = 0.000001\n",
        "  while np.linalg.norm(f(x)) > TOL:\n",
        "    df = scipy.misc.derivative(f, x)\n",
        "    x = x - f(x)/df\n",
        "    \n",
        "  return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcVfT_vkckWc",
        "colab_type": "text"
      },
      "source": [
        "In the code below there is unit tests that are used to assure the correctness of the implemented Jacobi iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgrwB4lhckmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scalar_function_one(x):\n",
        "  return x ** 3 + 1\n",
        "\n",
        "def scalar_function_two(x):\n",
        "  return (x - 3)**2\n",
        "\n",
        "class TestNewtonMethodScalar(unittest.TestCase):\n",
        "  def test_fx_one(self):\n",
        "\n",
        "    # Act\n",
        "    root = newtons_method_scalar(scalar_function_one)\n",
        "\n",
        "    # Assert\n",
        "    self.assertAlmostEqual(0, scalar_function_one(root), 2)\n",
        "\n",
        "  def test_fx_two(self):\n",
        "    # Act\n",
        "    root = newtons_method_scalar(scalar_function_two)\n",
        "\n",
        "    # Assert\n",
        "    self.assertAlmostEqual(0, scalar_function_two(root), 2)\n",
        "\n",
        "  def test_x_y_one(self):\n",
        "    # Arrange\n",
        "    expected_root = -1\n",
        "\n",
        "    # Act \n",
        "    root = newtons_method_scalar(scalar_function_one)\n",
        "\n",
        "    # Assert\n",
        "    self.assertAlmostEqual(0, abs(root - expected_root), 2)\n",
        "\n",
        "  def test_x_y_two(self):\n",
        "    # Arrange\n",
        "    expected_root = 3\n",
        "\n",
        "    # Act \n",
        "    root = newtons_method_scalar(scalar_function_two)\n",
        "\n",
        "    # Assert\n",
        "    self.assertAlmostEqual(0, abs(root - expected_root), 2)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Help from user Pierre S. in the stack overflow thread to give the main arguments: \n",
        "    # https://stackoverflow.com/questions/49952317/python3-for-unit-test-attributeerror-module-main-has-no-attribute-kerne \n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_",
        "colab_type": "text"
      },
      "source": [
        "# **Results**\n",
        "All the alogrithms are implemented and the test are passing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGKyAVKBu8ao",
        "colab_type": "code",
        "outputId": "4be8618c-88f0-4976-80ff-30cb9ce116a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # Help from user Pierre S. in the stack overflow thread to give the main arguments: \n",
        "    # https://stackoverflow.com/questions/49952317/python3-for-unit-test-attributeerror-module-main-has-no-attribute-kerne \n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "..........\n",
            "----------------------------------------------------------------------\n",
            "Ran 10 tests in 0.055s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m",
        "colab_type": "text"
      },
      "source": [
        "# **Discussion**\n",
        "The results of the algorithms are sufficient to say that the algorithms are likely correctly implemented. There is still room for some further improvements in shape of guards that prevents from users of the methods from using them wrongly, such as type checks in earliest parts of the function. Also even if the types in the arguments are correct, there could be illgal states of an object (if the argument is an object), such as the numpy array could have illegal dimensions to perform a matrix multiplication."
      ]
    }
  ]
}