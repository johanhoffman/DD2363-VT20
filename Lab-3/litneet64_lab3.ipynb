{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "litneet64_lab3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363-VT20/blob/litneet64/Lab-3/litneet64_lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 3: Iterative Methods**\n",
        "**Pablo Aravena**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6UFTSzW7P8kL"
      },
      "source": [
        " We are tasked with coding the implementations for 3 non-direct, iterative methods to solve the linear equation $Ax = b$ and the root finding problem $f(x^*) = 0$.  These were the Jacobi iteration and the Gauss-Siedel method, and the Newton method, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "# **About the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pdll1Xc9WP0e",
        "outputId": "9dc997f2-bb48-4e20-b484-aac638339104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2019 Pablo Aravena (pjan2@kth.se)\n",
        "\n",
        "# Based on the template by Johan Hoffman (jhoffman@kth.se)\n",
        "# This file is part of the course DD2363 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xw7VlErAX7NS",
        "colab": {}
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "import unittest\n",
        "import random as rd\n",
        "from sympy import diff\n",
        "from sympy.abc import x as sp_x\n",
        "from sympy.utilities.lambdify import lambdify\n",
        "from math import isclose"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fC7sWjPE2X9C"
      },
      "source": [
        "We implemented 3 iterative methods to solve the linear system $Ax = b$, and the equation $f(x^*) = 0$. Some caution was taken at the creation of the tests for the matrix related methods, as these associated matrices need to possess some specific conditions for these methods to converge. Hence some grooming was necessary for these randomly generated matrices. \n",
        "\n",
        "For the newton method, a small error tolerance and a big number of iterations was put to it to ensure good approximations for the tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WeFO9QMeUOAu"
      },
      "source": [
        "# **Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K8yzio7v2X9G"
      },
      "source": [
        "#### Jacobi method\n",
        "  Following what it's shown on the lecture notes on $7.3$ about the Jacobi Iteration, follows:\n",
        "\n",
        "\\begin{equation*}\n",
        "    x_i^{(n+1)} = \\frac{1}{a_{ii}} \\cdot \\left(b_i - \\sum_{i \\neq j} a_{ij}\\cdot x_j^{(n)} \\right)\n",
        "\\end{equation*}\n",
        "\n",
        "So we get a better approximation element-wise after each iteration, which we limited as $5.000$ if a good enough approximation (error less than $10^{-9}$) is not found by then."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wpmGIpV52khU",
        "colab": {}
      },
      "source": [
        "# method for the jacobi iteration\n",
        "def jacobiIter(A, b):\n",
        "    # iteration limit\n",
        "    iter_limit = 5000\n",
        "    \n",
        "    # zero vector\n",
        "    zero_vect = np.zeros((b.shape[0], b.shape[1]))\n",
        "    \n",
        "    # initial guess and iter counter\n",
        "    new_x = x = np.zeros((b.shape[0], b.shape[1]))\n",
        "    k = 0 \n",
        "    \n",
        "    # initial diff\n",
        "    error = False\n",
        "    \n",
        "    # while we don't converge with a good approximation\n",
        "    while not error and k < iter_limit:\n",
        "        new_x = np.zeros((b.shape[0], b.shape[1]))\n",
        "        \n",
        "        # go through every element that's not in the diagonal\n",
        "        for row in range(A.shape[0]):\n",
        "            sum_1 = 0\n",
        "            \n",
        "            # ... and make an element by element mult if indexes aren't equal\n",
        "            for col in range(A.shape[1]):\n",
        "                if row != col:\n",
        "                    sum_1 += A[row, col] * x[col, 0]\n",
        "            \n",
        "            # update value for x_n+1\n",
        "            new_x[row, 0] = (1 / A[row, row]) * (b[row, 0] - sum_1)\n",
        "        \n",
        "        \n",
        "        # increase iteration counter and update x for the next one\n",
        "        k += 1\n",
        "        x = new_x\n",
        "        \n",
        "        # re-calculate new error (gives out a boolean val)\n",
        "        error = np.allclose(np.dot(A, x) - b, zero_vect)\n",
        "    \n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qGBVXNRa2X9L"
      },
      "source": [
        "#### Gauss-Siedel Method\n",
        "\n",
        "This method is similar to the previous in both the error tolerance and iteration limit, except that the $A$ matrix is splitted into $2$ where one is the lower-triangular matrix and the other one the upper-triangular one (both diagonal exclusive). It follows then:\n",
        "\n",
        "\\begin{equation*}\n",
        "    x_i^{(n+1)} = \\frac{1}{a_{ii}} \\cdot \\left(b_i - \\sum_{i \\lt j} a_{ij}\\cdot x_j^{(n)} - \\sum_{i \\gt j} a_{ij}\\cdot x_j^{(n+1)} \\right)\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KB8MP7WX2X9N",
        "colab": {}
      },
      "source": [
        "# gauss-siedel\n",
        "def gaussSiedel(A, b):\n",
        "    # iteration limit\n",
        "    iter_limit = 5000\n",
        "    \n",
        "    # zero vector\n",
        "    zero_vect = np.zeros((b.shape[0], b.shape[1]))\n",
        "    \n",
        "    # create initial guess and iter counter\n",
        "    new_x = x = np.zeros((b.shape[0], b.shape[1]))\n",
        "    k = 0\n",
        "    \n",
        "    # initial error\n",
        "    error = False\n",
        "    \n",
        "    # run until iteration limit is reached or a good approximation is found\n",
        "    while not error and k < iter_limit:\n",
        "        for row in range(A.shape[0]):\n",
        "            sum_1 = sum_2 = 0\n",
        "        \n",
        "            for col in range(A.shape[1]):\n",
        "                # check for indexes for their corresponding sums\n",
        "                if row < col:\n",
        "                    sum_1 += A[row, col] * new_x[col, 0]\n",
        "                elif row > col:\n",
        "                    sum_2 += A[row, col] * x[col, 0]\n",
        "            \n",
        "            # update val on solution vector\n",
        "            x[row, 0] = (1 / A[row, row]) * (b[row, 0] - sum_1 - sum_2)\n",
        "        \n",
        "        \n",
        "            # re-calculate error and increase iter counter\n",
        "            error = np.allclose(np.dot(A, x) - b, zero_vect)\n",
        "            k += 1\n",
        "        \n",
        "    return x\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VJQ6ut_22X9R"
      },
      "source": [
        "#### Newton's method\n",
        "  For this method we just used:\n",
        "  \n",
        "\\begin{equation*}\n",
        "    x^{(n+1)} = x^{(n)} - \\frac{f(x^{(n)})}{f^{'}(x^{(n)})}\n",
        "\\end{equation*}\n",
        "\n",
        "A limit of $100.000$ iterations was placed in case the approximation didn't reach a close enough value to the root within an error margin of $10^{-30}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SL0qNweV2X9T",
        "colab": {}
      },
      "source": [
        "# newton's method\n",
        "def newtonMethod(func):\n",
        "    # iteration limit\n",
        "    iter_limit = 100_000\n",
        "    \n",
        "    # get the derivative of our function\n",
        "    temp_diff = diff(func(sp_x), sp_x)\n",
        "    d_func = lambdify(sp_x, temp_diff)\n",
        "    \n",
        "    # initial guess and iter counter\n",
        "    x = new_x = 0\n",
        "    k = 0\n",
        "    \n",
        "    while (abs(func(x)) > 1e-30) and k < iter_limit:\n",
        "        x = new_x - func(new_x) / d_func(new_x)\n",
        "        \n",
        "        # update new x and increase iteration counter\n",
        "        new_x = x\n",
        "        k += 1\n",
        "        \n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jtUv3B0u2X9n"
      },
      "source": [
        "#### Tester class\n",
        "\n",
        "We implemented a class inheriting from the `unittest` framework as to run some test cases at the end. These are bulk tests with randomized dimensions and elements for the matrices, and randomized coefficients for the cubic polynomials used to test the Newton Method. A high-order helper function was created as to help in reusing code for common patterns. The chosen number of test cases is $1.000$. \n",
        "\n",
        "For the tests that required matrices, the numpy methods `allclose` and `linalg.solve` were used to get the real solutions and also to compare them with our given ones. One important detail was to actually create an $A$ matrix that would make these methods converge. We know that a sufficient condition for both methods to converge is that the $A$ matrix used should be strictly diagonal dominant, that is if for every row:\n",
        " \n",
        "\\begin{equation*}\n",
        "    |a_{ii}| > \\sum_{i \\neq j} |a_{ij}|\n",
        "\\end{equation*}\n",
        "\n",
        "As we are just using positive real numbers for the elements of these matrices, the absolute value is not required in our `makeDiagonalDom` function.\n",
        "\n",
        "To test the Newton method we just evaluated the given root of that polynomial inside it and compared it to $0$, as $f(x^*) = 0$, where $x^*$ is a root, and we used the method `isclose` for this, which has a tolerance for errors of $10^{-10}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E6ehOLuC2X9q",
        "colab": {}
      },
      "source": [
        "# generic tester class\n",
        "class method_tester(unittest.TestCase):\n",
        "    # number of test cases and maximum number of rows/cols\n",
        "    TEST_CASES = 1000\n",
        "    MAX_ROWS = 10\n",
        "    \n",
        "    # maximum for polynomial coeffs\n",
        "    MAX_COEFF = 30\n",
        "    \n",
        "    # make a matrix diagonal dominant\n",
        "    def makeDiagonalDom(self, A):\n",
        "        # go through every row\n",
        "        for i in range(A.shape[0]):\n",
        "            # get the current row and the sum of it's elements\n",
        "            curr_row = A[i,]\n",
        "            row_sum = sum(curr_row)\n",
        "            \n",
        "            # replace the diagonal element with a big enough one\n",
        "            A[i,i] = row_sum + 2\n",
        "    \n",
        "    \n",
        "    # general bulk tester for matrix methods\n",
        "    def bulkHelper(self, matrix_method):\n",
        "        # go through 1.000 test cases\n",
        "        for i in range(self.TEST_CASES):\n",
        "            # get random row and column dimensions (should be a square matrix)\n",
        "            rows_cols = rd.randint(2, self.MAX_ROWS + 1)\n",
        "            \n",
        "            # get a random matrix and condition it to make these methods converge\n",
        "            A = np.random.rand(rows_cols, rows_cols) * 2    \n",
        "            self.makeDiagonalDom(A)\n",
        "            \n",
        "            # generate random vector (should be vertical for our functions)\n",
        "            b = np.random.rand(rows_cols, 1) * 5\n",
        "            \n",
        "            # test the method with the corresponding real value given by numpy (error tolerance of 1e-05)\n",
        "            self.assertTrue(np.allclose(matrix_method(A, b), np.linalg.solve(A, b)))\n",
        "            \n",
        "    \n",
        "    # random bulk tester for the jacobi iteration method\n",
        "    def testJacobi(self):\n",
        "        self.bulkHelper(jacobiIter)\n",
        "        \n",
        "    # random bulk tester for the Gauss-Siedel iteration method\n",
        "    def testGS(self):\n",
        "        self.bulkHelper(gaussSiedel)\n",
        "        \n",
        "    # random bulk tester for Newton's method\n",
        "    def testNewton(self):\n",
        "        # go through 1.000 test cases \n",
        "        for i in range(self.TEST_CASES):\n",
        "            # list holding the coefficients for our cubic polynomial\n",
        "            coeffs = [0 for i in range(4)]\n",
        "            \n",
        "            for i in range(4):\n",
        "                coeffs[i] = rd.randint(1, self.MAX_COEFF + 1)\n",
        "                \n",
        "            # generate cubic polynomial\n",
        "            pol = lambda x: coeffs[0]*x**3 + coeffs[1]*x**2 + coeffs[2]*x - coeffs[3]\n",
        "            \n",
        "            root = newtonMethod(pol)\n",
        "            \n",
        "            # the function evaluated on it's own root should be 0 (f(x_0) == 0) (error tolerance of 1e-10)\n",
        "            self.assertTrue(isclose(abs(pol(root)), 0, abs_tol = 1e-10))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RLwlnOzuV-Cd"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "The results shows that no errors were caused by the tests, proving they work as intended. It's interesting to note that even though we needed to create randomized matrices, both dimension-wise and element-wise, this was not enough to ensure that our methods would converge, thus some modifications were needed on these matrices before being used for testing. "
      ]
    }
  ]
}