{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iZafiro_lab3",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7",
        "colab_type": "text"
      },
      "source": [
        "# **Lab 3: Iterative methods**\n",
        "**Fabi치n Levic치n**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm",
        "colab_type": "text"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL",
        "colab_type": "text"
      },
      "source": [
        "This is the third lab in the course DD2363 Methods in Scientific Computing. It is about using Jupyter to implement four iterative methods for solving linear and non-linear equations. Some objectives may be to become familiar with the differences between \"discrete\" and \"continuous\" methods, and also between direct and iterative methods. The functions implemented are jacobi, gaussSeidel, newtonScalar, and newtonVector.  The residuals of the solutions are then compared to the exact solutions of a few equations, and the results are favorable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3",
        "colab_type": "text"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab_type": "code",
        "outputId": "66801a6f-20aa-42be-df9b-0e4a76cd278d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2020 Fabi치n Levic치n (fils2@kth.se)\n",
        "\n",
        "# This file is part of the course DD2363 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh",
        "colab_type": "text"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import math\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6",
        "colab_type": "text"
      },
      "source": [
        "Iterative methods are (in general) algorithms that, given an equation and possibly an initial solution and a number of iterations, return an approximation of the solution to the equation. They are called iterative because the $k$-th step uses the solution obtained in the $(k-1)$-th step, and the solutions (ideally) form a sequence that is convergent to the exact solution.\n",
        "\n",
        "The following are implementations of the iterative methods mentioned in the abstract. Linear equations of the form $Ax = b$, and non-linear equations of the form $f(x) = 0$, are henceforth assumed. The author decided to implement newtonScalar and newtonVector spearately, although he is aware that one is a special case of the other.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeFO9QMeUOAu",
        "colab_type": "text"
      },
      "source": [
        "# **Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx",
        "colab_type": "text"
      },
      "source": [
        "The method jacobi takes a matrix $A$ and a vector $b$ as input and returns None if the matrix is not square, if the matrix has a zero entry in the diagonal, if the residual $\\|Ax - b\\|$ becomes too large, or if the method does not converge after a certain number of iterations and the residual is still too large. The author decided not to consider more sophisticated criteria for convergence, as he couldn't find a necessary and sufficient condition for it (only either necessary or sufficient conditions). Instead, the method may halt in the aforementioned way. The method returns an approximate solution $x$ if the residual is small enough.\n",
        "\n",
        "Two copies of the solution are stored and used in every iteration. Every entry of the most recent copy at every iteration is updated according to the formula $x_i^{(k + 1)} = \\frac{1}{a_{ii}}\\left (b_i - \\sum_{j \\neq i} a_{ij}x_j^{(k)}\\right )$, which follows from the Jacobi iteration $x^{(k + 1)} = D^{- 1}(b - Rx^{(k)})$, where $D$ is the diagonal of $A$ and $R$ is the matrix containing the rest of the entries of $A$ (and $0$s elsewhere).\n",
        "\n",
        "The initial solution is assumed to be the zero vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOtxTd49EGq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jacobi(A, b):\n",
        "  # Initialize variables\n",
        "  # The initial solution is assumed to be the zero vector\n",
        "  # The algorithm halts if the residual lies outside the interval [epsilon, maxResidual]\n",
        "  # or if its is greater than maxIts = 10000\n",
        "  n = A.shape[0]\n",
        "  x1 = np.zeros(n)\n",
        "  x2 = np.zeros(n)\n",
        "  residual = 1\n",
        "  epsilon = 0.0001\n",
        "  maxResidual = 10000\n",
        "  maxIts = 10000\n",
        "  its = 0\n",
        "\n",
        "  # Basic error handling\n",
        "  if(A.shape[0] != A.shape[1]):\n",
        "    print(\"Error: The matrix is not square.\")\n",
        "    return None\n",
        "  flag = False\n",
        "  for i in range(n):\n",
        "    if(A[i][i] == 0):\n",
        "      flag = True\n",
        "  if(flag):\n",
        "    print(\"Error: The matrix has a zero entry in the diagonal.\")\n",
        "    return None\n",
        "\n",
        "  while(residual >= epsilon and residual <= maxResidual and its <= maxIts):\n",
        "    # Update x2 (i. e., x_(k+1))\n",
        "    for i in range(n):\n",
        "      sum = 0\n",
        "      for j in range(n):\n",
        "        if(j != i):\n",
        "          sum += A[i][j]*x1[j]\n",
        "      x2[i] = 1.0/A[i][i]*(b[i] - sum)\n",
        "\n",
        "    # Update x1 (i. e., x_k)\n",
        "    for i in range(n):\n",
        "      x1[i] = x2[i]\n",
        "\n",
        "    # Calculate the residual\n",
        "    residual = np.linalg.norm((A @ x2) - b)\n",
        "\n",
        "    # Update its\n",
        "    its += 1\n",
        "\n",
        "  # Return the approximate solution\n",
        "  if(np.linalg.norm((A @ x2) - b) < epsilon):\n",
        "    return x2\n",
        "  else:\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2cCsV-tpeGZ",
        "colab_type": "text"
      },
      "source": [
        "The method gaussSeidel is almost identical to the method jacobi, except that only one copy of the solution is stored and used. This is because every entry at every iteration is updated according to the formula $x_i^{(k + 1)} = \\frac{1}{a_{ii}}\\left (b_i - \\sum_{j < i} a_{ij}x_j^{(k + 1)} - \\sum_{j > i} a_{ij}x_j^{(k)}\\right )$ which follows from using forward substitution on the Gauss-Seidel iteration $Lx^{(k + 1)} = b - Ux^{(k)}$, and if the entries of the solution are updated in increasing order, it is easy to see that the formula doesn't use any entries of the previous solution that have already been updated (i. e., it only uses entries of the current solution, and entries of the previous solution that haven't been updated yet). $L$ is the lower triangular component of $A$ and $U$ is the upper triangular component of $A$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBPhBA8ZLIRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gaussSeidel(A, b):\n",
        "  # Initialize variables\n",
        "  # The initial solution is assumed to be the zero vector\n",
        "  # The algorithm halts if the residual lies outside the interval [epsilon, maxResidual]\n",
        "  # or if its is greater than maxIts = 10000\n",
        "  n = A.shape[0]\n",
        "  x = np.zeros(n)\n",
        "  residual = 1\n",
        "  epsilon = 0.0001\n",
        "  maxResidual = 10000\n",
        "  maxIts = 10000\n",
        "  its = 0\n",
        "\n",
        "  # Basic error handling\n",
        "  if(A.shape[0] != A.shape[1]):\n",
        "    print(\"Error: The matrix is not square.\")\n",
        "    return None\n",
        "  flag = False\n",
        "  for i in range(n):\n",
        "    if(A[i][i] == 0):\n",
        "      flag = True\n",
        "  if(flag):\n",
        "    print(\"Error: The matrix has a zero entry in the diagonal.\")\n",
        "    return None\n",
        "\n",
        "  while(residual >= epsilon and residual <= maxResidual and its <= maxIts):\n",
        "    # Update x\n",
        "    for i in range(n):\n",
        "      sum = 0\n",
        "      for j in range(n):\n",
        "        if(j != i):\n",
        "          sum += A[i][j]*x[j]\n",
        "      x[i] = 1.0/A[i][i]*(b[i] - sum)\n",
        "    \n",
        "    # Calculate the residual\n",
        "    residual = np.linalg.norm((A @ x) - b)\n",
        "\n",
        "    # Update its\n",
        "    its += 1\n",
        "  \n",
        "  # Return the approximate solution\n",
        "  if(np.linalg.norm((A @ x) - b) < epsilon):\n",
        "    return x\n",
        "  else:\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5-pD95ltyj_",
        "colab_type": "text"
      },
      "source": [
        "The method newtonScalar takes a function $f$ and its first derivative (which is also a function) as inputs and returns None if the derivative at some iteration is $0$ and the evaluation $|f(x)|$ is still too large, or if the method doesn't converge after a certain number of iterations and the evaluation is still too large. The method returns an approximate solution $x$ if the evaluation is small enough.\n",
        "\n",
        "At every iteration, the solution is updated according to the formula $x^{(k + 1)} = x^{(k)} - \\frac{f(x^{(k)})}{f'(x^{(k)})}$.\n",
        "\n",
        "The initial solution is assumed to be zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBuaXi21QK41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def newtonScalar(func, derivative):\n",
        "  # Initialize variables\n",
        "  # The initial solution is assumed to be zero\n",
        "  # The algorithm halts if the absolute value of func(x) is less than epsilon = 0.0001\n",
        "  # or if its is greater than maxIts = 10000\n",
        "  x = 0\n",
        "  absfx = 1\n",
        "  epsilon = 0.0001\n",
        "  maxIts = 10000\n",
        "  its = 0\n",
        "\n",
        "  while(absfx >= epsilon and its <= maxIts):\n",
        "    # Update iteration variables\n",
        "    funcx = func(x)\n",
        "    derivativex = derivative(x)\n",
        "\n",
        "    # Check if the derivative at x is zero\n",
        "    if(derivativex == 0):\n",
        "      print(\"Error: The derivative at iteration \" + str(its) + \" is zero, and thus the algorithm can't continue.\")\n",
        "      break\n",
        "    \n",
        "    # Update x\n",
        "    x -= funcx/derivativex\n",
        "\n",
        "    # Update absfx\n",
        "    absfx = abs(funcx)\n",
        "\n",
        "    # Update its\n",
        "    its += 1\n",
        "  \n",
        "  # Return the approximate root\n",
        "  if(abs(func(x)) < epsilon):\n",
        "    return x\n",
        "  else:\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh9oq8_Mv-uk",
        "colab_type": "text"
      },
      "source": [
        "The method newtonVector is almost identical to the method newtonScalar, except that the function $f$ is a map from $\\mathbb{R}^n$ to $\\mathbb{R}^n$, and the derivative is instead its Jacobian matrix $J$, which is a map from $\\mathbb{R}^n$ to $\\mathbb{R}^{n \\times n}$. These functions work as usual Python functions. Naturally, the evaluation is $\\|f(x)\\|$. The solution is updated according to the formula $x^{(k + 1)} = x^{(k)} - J^{-1}(x^{(k)})\\frac{f(x^{(k)})}{f'(x^{(k)})}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkmglrVLnUDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def newtonVector(func, JacobianMatrix, n):\n",
        "  # Initialize variables\n",
        "  # The initial solution is assumed to be the zero vector\n",
        "  # The algorithm halts if the norm of func(x) is less than epsilon = 0.0001\n",
        "  # or if its is greater than maxIts = 10000\n",
        "  x = np.zeros(n)\n",
        "  normfx = 1\n",
        "  epsilon = 0.0001\n",
        "  maxIts = 10000\n",
        "  its = 0\n",
        "\n",
        "  while(normfx >= epsilon and its <= maxIts):\n",
        "    # Update iteration variables\n",
        "    funcx = func(x)\n",
        "    JacobianMatrixx = JacobianMatrix(x)\n",
        "\n",
        "    # Check if the Jacobian matrix at x is not invertible\n",
        "    if(np.linalg.det(JacobianMatrixx) == 0):\n",
        "      print(\"Error: The Jacobian matrix at iteration \" + str(its) + \" is not invertible, and thus the algorithm can't continue.\")\n",
        "      break\n",
        "\n",
        "    # Update x\n",
        "    x -= np.linalg.inv(JacobianMatrixx) @ funcx\n",
        "\n",
        "    # Update normfx\n",
        "    normfx = np.linalg.norm(funcx)\n",
        "\n",
        "    # Update its\n",
        "    its += 1\n",
        "\n",
        "  # Return the approximate root\n",
        "  if(np.linalg.norm(func(x)) < epsilon):\n",
        "    return x\n",
        "  else:\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_",
        "colab_type": "text"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5nyFG0nUmcG",
        "colab_type": "text"
      },
      "source": [
        "The methods jacobi and gaussSeidel are tested with various systems of linear equations. Both methods should diverge with the three first systems, and both methods should converge with the three final systems. Both methods should print an error with the system that has a zero entry in the diagonal. When the methods converge, the quantity $\\|x - y\\|$, where $x$ is the returned vector and $y$ is a vector known to be close enough to the solution, is asserted to be less than epsilon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGfm5lO5xp6q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "d9c5f795-eab8-420b-d251-c2106b567f88"
      },
      "source": [
        "# This matrix has a zero entry in the diagonal\n",
        "A1 = np.array([[-1.0, 0.0, 1.0], [0.0, 0.0, 0.0], [1.0, 0.0, -1.0]])\n",
        "b1 = np.array([-1.0, 1.0, -1.0])\n",
        "\n",
        "# The methods should diverge for this matrix\n",
        "A2 = np.array([[-2, 3, -5, 7, -11], [13, -17, 19, -23, 29], [-31, 37, -41, 43, -47], [53, -59, 61, -67, 71], [-73, 79, -83, 89, -97]])\n",
        "b2 = np.array([42, 42, 42, 42, 42])\n",
        "\n",
        "# The methods should diverge for this matrix (from Wikipedia)\n",
        "A3 = np.array([[2.0, 3.0], [5.0, 7.0]])\n",
        "b3 = np.array([11.0, 13.0])\n",
        "\n",
        "# The methods should converge for this matrix (from Wikipedia)\n",
        "A4 = np.array([[2.0, 1.0], [5.0, 7.0]])\n",
        "b4 = np.array([11.0, 13.0])\n",
        "y4 = np.array([7.1111, -3.2222])\n",
        "\n",
        "# The methods should converge for this matrix (from Wikipedia)\n",
        "A5 = np.array([[16.0, 3.0], [7.0, -11.0]])\n",
        "b5 = np.array([11.0, 13.0])\n",
        "y5 = np.array([0.8122, -0.6650])\n",
        "\n",
        "# This matrix should converge for this matrix (from Wikipedia)\n",
        "A6 = np.array([[10.0, -1.0, 2.0, 0.0], [-1.0, 11.0, -1.0, 3.0], [2.0, -1.0, 10.0, -1.0], [0.0, 3.0, -1.0, 8.0]])\n",
        "b6 = np.array([6.0, 25.0, -11.0, 15.0])\n",
        "y6 = np.array([1.0, 2.0, -1.0, 1.0])\n",
        "\n",
        "epsilon = 0.0001\n",
        "assert jacobi(A1, b1) == None\n",
        "assert gaussSeidel(A1, b1) == None\n",
        "assert jacobi(A2, b2) == None\n",
        "assert gaussSeidel(A2, b2) == None\n",
        "assert jacobi(A3, b3) == None\n",
        "assert np.linalg.norm(jacobi(A4, b4) - y4) < epsilon\n",
        "assert np.linalg.norm(gaussSeidel(A4, b4) - y4) < epsilon\n",
        "assert np.linalg.norm(jacobi(A5, b5) - y5) < epsilon\n",
        "assert np.linalg.norm(gaussSeidel(A5, b5) - y5) < epsilon\n",
        "assert np.linalg.norm(jacobi(A6, b6) - y6) < epsilon\n",
        "assert np.linalg.norm(gaussSeidel(A6, b6) - y6) < epsilon\n",
        "print(\"Tests passed successfully!\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: The matrix has a zero entry in the diagonal.\n",
            "Error: The matrix has a zero entry in the diagonal.\n",
            "Tests passed successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o44bsBLKWGOY",
        "colab_type": "text"
      },
      "source": [
        "The method newtonScalar is tested with:\n",
        "1. $f(x) = x^2 + 1$, $f'(x) = 2x$. As $f'(x_0) = 0$, the method should print an error, and return None.\n",
        "2. $f(x) = cos(x)$, $f'(x) = -sin(x)$. Idem.\n",
        "3. $f(x) = sin(x)$, $f'(x) = cos(x)$. As $f(x_0) = 0$, the method should return $0$ after iteration $0$.\n",
        "4. $f(x) = x^3 + x$, $f'(x) = 3x^2 + 1$. As $f(x_0) = 0$, the method should return $0$ after iteration $0$.\n",
        "5. $f(x) = x - \\pi$, $f'(x) = 1$. The method should converge to $\\pi$.\n",
        "6. $f(x) = cos(x) - x$, $f'(x) = -sin(x) - 1$. The method should converge to the cosine fixed point constant.\n",
        "\n",
        "When the method converges (i. e., when it doesn't return None), the quantity $|x - y|$, where $x$ is the returned value and $y$ is a value known to be close enough to the solution, is asserted to be less than epsilon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qaK1LSlAbds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "ec0ab52d-b547-4fd3-e70a-ddce6959a311"
      },
      "source": [
        "epsilon = 0.0001\n",
        "assert newtonScalar(lambda x: x**2 + 1, lambda x: 2*x) == None\n",
        "assert newtonScalar(math.cos, lambda x: -math.sin(x)) == None\n",
        "assert abs(newtonScalar(math.sin, math.cos) - 0.0) < epsilon\n",
        "assert abs(newtonScalar(lambda x: x**3 + x, lambda x: 3*x**2 + 1) - 0.0) < epsilon\n",
        "assert abs(newtonScalar(lambda x: x - math.pi, lambda x: 1) - math.pi) < epsilon\n",
        "assert abs(newtonScalar(lambda x: math.cos(x) - x, lambda x: -math.sin(x) - 1) - 0.7391) < epsilon\n",
        "print(\"Tests passed successfully!\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: The derivative at iteration 0 is zero, and thus the algorithm can't continue.\n",
            "Error: The derivative at iteration 0 is zero, and thus the algorithm can't continue.\n",
            "Tests passed successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6ijV0PUgZGp",
        "colab_type": "text"
      },
      "source": [
        "The method newtonVector is tested with:\n",
        "1.  $f(x, y, z) = (cos(x), x + y + z, x + y + z)$, $J(x, y, z) = \\{\\{-sin(x), 0, 0\\}, \\{1, 1, 1\\}, \\{1, 1, 1\\}\\}$. As $-sin(0) = 0$, the method should print an error, and return None.\n",
        "2.  $f(x, y) = (2x - 3y + 5, 4x - 7y + 10)$, $J(x, y) = \\{\\{2, -3\\}, \\{4, -7\\}\\}$. The method should converge to $(-2.5, 0)$.\n",
        "3.  $f(x, y) = (x - 2)^2 + (y - 8)^2 = 40$, $J(x, y) = \\{\\{2(x - 2), 2(y - 8)\\}, \\{-2, 1\\}\\}$. The method should converge to $(4, 2)$.\n",
        "4.  $f(x, y) = (e^x - 1, cos(y) - 1)$, $J(x, y) = \\{\\{e^x, 0\\}, \\{0, -sin(y)\\}\\}$. As $sin(0) = 0$, the method should print an error, but because $f(0, 0) = (0, 0)$, the method should return $(0, 0)$ after iteration $0$.\n",
        "\n",
        "When the method converges (i. e., when it doesn't return None), the quantity $\\|x - y\\|$, where $x$ is the returned vector and $y$ is a vector known to be close enough to the solution, is asserted to be less than epsilon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj1wgH0qGUmB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "ed5527fe-4caf-4db2-c3e5-f96e8f077e84"
      },
      "source": [
        "epsilon = 0.0001\n",
        "assert newtonVector(lambda x: np.array([math.cos(x[0]), x[0] + x[1] + x[2], x[0] + x[1] + x[2]]), lambda x: np.array([[-math.sin(x[0]), 0, 0], [1, 1, 1], [1, 1, 1]]), 3) == None\n",
        "assert np.linalg.norm(newtonVector(lambda x: np.array([2.0*x[0] - 3.0*x[1] + 5.0, 4.0*x[0] - 7.0*x[1] + 10.0]), lambda x: np.array([[2.0, -3.0], [4.0, -7.0]]), 2) - np.array([-2.5, 0])) < epsilon\n",
        "# (from lumenlearning.com)\n",
        "assert np.linalg.norm(newtonVector(lambda x: np.array([(x[0] - 2)**2 + (x[1] - 8)**2 - 40, -2*x[0] + x[1] +6]), lambda x: np.array([[2*(x[0] - 2), 2*(x[1] - 8)], [-2, 1]]), 2) - np.array([4.0, 2.0])) < epsilon\n",
        "assert np.linalg.norm(newtonVector(lambda x: np.array([math.exp(x[0]) - 1.0, math.cos(x[1]) - 1]), lambda x: np.array([[math.exp(x[0]), 0.0], [0.0, -math.sin(x[1])]]), 2) - np.array([0.0, 0.0])) < epsilon\n",
        "print(\"Tests passed successfully!\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: The Jacobian matrix at iteration 0 is not invertible, and thus the algorithm can't continue.\n",
            "Error: The Jacobian matrix at iteration 0 is not invertible, and thus the algorithm can't continue.\n",
            "Tests passed successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd",
        "colab_type": "text"
      },
      "source": [
        "All the tests pass successfully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m",
        "colab_type": "text"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe",
        "colab_type": "text"
      },
      "source": [
        "The author thinks it is surprising that the Gauss-Seidel method is in a very concrete way simpler than the Jacobi method, and yet it is theoretically faster. The author also thinks the treatment of lambda functions in python is extremely intuitive and useful. The results were favourable and expected. \n",
        "\n",
        "The algorithms here presented could be improved by adding more sophisticated criteria for convergence, such as considering the spectral radius for the linear methods, and the norm of the derivative or Jacobian matrix for the non-linear methods.\n",
        "\n",
        "The class notes were extensively consulted. The Jacobi method and Gauss-Seidel method articles in the English Wikipedia were used while writing this document. [This](https://https://www.lakeheadu.ca/sites/default/files/uploads/77/docs/RemaniFinal.pdf) paper was also used for the part on the Newton method for vector functions. One system of non-linear equations was obtained from a webpage on [this](http://lumenlearning.com) website.\n",
        "\n",
        "The author collaborated with Pablo Aravena and Felipe Vicencio."
      ]
    }
  ]
}