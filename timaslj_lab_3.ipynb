{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "timaslj-lab-3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363-VT20/blob/timaslj/timaslj_lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7",
        "colab_type": "text"
      },
      "source": [
        "# **Lab 2: Matrix factorization**\n",
        "**Timas Ljungdahl**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm",
        "colab_type": "text"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL",
        "colab_type": "text"
      },
      "source": [
        "In this report, 4 algorithms for solving equations iteratively were implemented and tested. The algorithms were Jocobi iteration, Gauss-Seidel iteration, Newton's method for scalar functions, and Newton's method for vector functions. All algorithms were tested with random data and generally generated desired results with around 8 decimal precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3",
        "colab_type": "text"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo",
        "colab_type": "text"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is distributed under a certain license. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab_type": "code",
        "outputId": "f74fa781-413b-41e7-a2ec-1bba2288ad4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2019 Johan Hoffman (jhoffman@kth.se)\n",
        "\n",
        "# This file is part of the course DD2363 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh",
        "colab_type": "text"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa",
        "colab_type": "text"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import unittest\n",
        "import random\n",
        "import math\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6",
        "colab_type": "text"
      },
      "source": [
        "In this report, systems of linear equations on the form $Ax = b$, non-linear scalar functions on the form $ \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ and non-linear vector functions on the form $ \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n} $ are investigated. \n",
        "\n",
        "In the case of very large or sparse system of linear equations $Ax = b$, it makes sense to use iterative methods instead of direct methods, as we want to avoid computing $A^{-1}$ as it can be computationally expensive. Instead we can use iterative methods to generate a sequence of approximations where the error will decrease every iteration. \n",
        "\n",
        "One type of such iterative methods is Richardson iteration that we formulate as $X^{k+1} = (I-\\alpha A)x^{k} + \\alpha b$ where $M = I - \\alpha A$ is called the iteration matrix. The method will converge if $||M|| < 1$. In this lab, two iterative methods based on the same theory are implemented; Jacobi and Gauss-Seidel method. The iteration continues until the residual ||$Ax_{approx} - b|| < TOL$ where $TOL$ is a small number close to $0$.  \n",
        "\n",
        "The Jacobi method is based on matrix splitting $A = D + R$ where $D$ is chosen to be a diagonal matrix which is easy to invert. The approximate solution to $Ax=b$ can then iteratively be obtained with $X^{k+1} = (I-D^{-1} A)x^{k} + D^{-1}b$. In this case $M = I-D^{-1}A$. \n",
        "\n",
        "The Gauss-Seidel method is also based on matrix splitting $A = L + U$ where $L$ is a lower triangular matrix with none-zero diagonal elements. The approximate solution to $Ax=b$ can then iteratively be obtained with $X^{k+1} = (I-L^{-1} A)x^{k} + L^{-1}b$. In this case $M = I-L^{-1}A$.  \n",
        "\n",
        "In order to iteratively solve for a root of a non-linear scalar function, Newton's method was implemented. It is based on computing the functions value and derivate in a point $x$ where the iteration becomes $x_{n+1} = x_n - f(x_n)/f^\\prime(x_n)$ where $x_{n+1}$ is a better approximation than $x_n$. In the case of a multivariate function, the Jacobian is computed. If $f(x)$ is a vector function the iteration becomes $x_{n+1} = x_n - f^\\prime(x_n)^{-1}f(x_n)$ where $f^\\prime(x_n)^{-1}$ is the inverse Jacobian matrix. This can be rewritten as $f^\\prime(x_n)^{-1}(x_{n+1}-x_n) = f(x_n)$ which resembles a system of linear equation of the form $Ax = b$, where $\\Delta x_{n+1} = x_{n+1}- x_n$ can be solved for.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeFO9QMeUOAu",
        "colab_type": "text"
      },
      "source": [
        "# **Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx",
        "colab_type": "text"
      },
      "source": [
        "All methods were implemented according to theory. \n",
        "\n",
        "For the Newton's methods, the approximate Jacobian is computed by computing the function value difference when inputing a slightly different $x$. \n",
        "\n",
        "The matrix iteration methods were tested by asserting the residual $||Ax_{approx} - b||$ and $||x_{approx}-y||$ to be 0, where $y$ is the exact solution, for 1000 random vectors. In order to ensure that the system converges, the system was preconditioned. \n",
        "\n",
        "For the Newtons method for scalar functions, random functions were generated and the residual $|f(x)|$ were asserted to 0. $|x-y|$ were very hard to test as the exact solution for the random functions were hard to obtain. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_",
        "colab_type": "text"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS2U_WWT15_4",
        "colab_type": "code",
        "outputId": "7e51a9f9-9d13-4b8f-dfcf-e4893af29ac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "##can be done without saving R,D and D_inverse -> much more space efficient\n",
        "def jacobi_iteration(A,b, TOL = 1e-8):\n",
        "  n = A.shape[0]  \n",
        "  D_inverse = np.zeros((n,n))\n",
        "  for i in range(n):\n",
        "    D_inverse[i,i] = 1/A[i,i]\n",
        "\n",
        "  M = np.identity(n) - np.matmul(D_inverse, A)\n",
        "\n",
        "  c = np.matmul(D_inverse,b)\n",
        "\n",
        "  x = np.random.rand(n)\n",
        "\n",
        "  while np.linalg.norm(np.matmul(A,x)-b) > TOL:\n",
        "    x = np.matmul(M, x) + c\n",
        "\n",
        "  return x\n",
        "\n",
        "def gauss_seidel_iteration(A, b, TOL = 1e-8):\n",
        "  n = A.shape[0]  \n",
        "  L_inverse = A.copy()\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      if (j > i):\n",
        "        L_inverse[i,j] = 0\n",
        "\n",
        "  L_inverse = np.linalg.inv(L_inverse)\n",
        "\n",
        "  M = np.identity(n) - np.matmul(L_inverse, A)\n",
        "\n",
        "  c = np.matmul(L_inverse, b)\n",
        "\n",
        "  x = np.random.rand(n)\n",
        "\n",
        "  while np.linalg.norm(np.matmul(A,x)-b) > TOL:\n",
        "    x = np.matmul(M, x) + c\n",
        "\n",
        "  return x\n",
        "    \n",
        "class Test(unittest.TestCase):\n",
        "\n",
        "  def test_random_vectors(self):\n",
        "    for n in range(1000):\n",
        "      size = random.randint(2, 50) \n",
        "      A = np.random.rand(size, size)\n",
        "      #left preconditioning so system converges\n",
        "      alpha = 1.1\n",
        "      B = np.linalg.inv(A)*alpha #approx inverse of A so ||I-alpha*B*A|| < 1\n",
        "      C = np.matmul(A,B)\n",
        "      x = np.random.rand(size)\n",
        "      b = np.matmul(C,x)\n",
        "\n",
        "      x_gauss_approx = gauss_seidel_iteration(C,b)\n",
        "      x_jacobi_approx = jacobi_iteration(C,b)\n",
        "\n",
        "      x_exact = np.linalg.solve(C,b)\n",
        "    \n",
        "      #||x-y||\n",
        "      self.assertAlmostEqual(np.linalg.norm(x_gauss_approx-x_exact), 0, 9)\n",
        "      self.assertAlmostEqual(np.linalg.norm(x_jacobi_approx-x_exact), 0, 9)\n",
        "\n",
        "      b_gauss_approx = np.matmul(C,x_gauss_approx)\n",
        "      b_jacobi_approx = np.matmul(C, x_jacobi_approx)\n",
        "\n",
        "      #||Ax-b||\n",
        "      self.assertAlmostEqual(np.linalg.norm(b_gauss_approx-b), 0, 9)\n",
        "      self.assertAlmostEqual(np.linalg.norm(b_jacobi_approx-b), 0, 9)\n",
        "\n",
        "    #print(x)\n",
        "    #x_prim = jacobi_iteration(A,b)\n",
        "    \n",
        "    \n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.858s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMcBhgAGVAIb",
        "colab_type": "code",
        "outputId": "d0c0b3b3-924c-46d1-b465-aed1275abb0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "def scalar_jacobian(f, x, dx=1e-8):\n",
        "  fx = f(x)\n",
        "  J = np.zeros(x.shape[0])\n",
        "  dxi = x.copy()\n",
        "\n",
        "  for i in range(len(x)):\n",
        "    dxi[i] = dxi[i] + dx\n",
        "    J[i] = abs(f(dxi)-fx)/dx \n",
        "    dxi[i] = x[i]\n",
        "  \n",
        "  return J\n",
        "\n",
        "def scalar_newtons_method(f, x0, TOL = 1e-8):\n",
        "  x = x0\n",
        "\n",
        "  while abs(f(x)) > TOL:\n",
        "    df = scalar_jacobian(f, x)\n",
        "    for i in range(x.shape[0]): \n",
        "      if not(math.isclose(df[i],0)): #divide by zero\n",
        "        x[i] -= f(x)/df[i]\n",
        "      else:\n",
        "        return None \n",
        "      \n",
        "  return x\n",
        "\n",
        "class Test(unittest.TestCase):\n",
        "  def get_random_func(n):\n",
        "    exponents = []\n",
        "    scalar = random.randint(0,1000)\n",
        "    for i in range(n):\n",
        "      exp = random.randint(1,5)\n",
        "      exponents.append(exp)\n",
        "\n",
        "    def random_func(x):\n",
        "      sum = 0\n",
        "      for i in range(n):\n",
        "        sum += x[i]**exponents[i]\n",
        "      return sum + scalar\n",
        "    return random_func\n",
        "\n",
        "  def test_random_functions(self):\n",
        "    for n in range(100000):\n",
        "      size = random.randint(2,10)\n",
        "      f = get_random_func(size)\n",
        "      x0 = np.zeros(size, dtype='float64')\n",
        "\n",
        "      root_approx = scalar_newtons_method(f,x0)\n",
        "\n",
        "      if root_approx is None:\n",
        "        continue\n",
        "\n",
        "      f_root = f(root_approx)\n",
        "    \n",
        "      if f_root == f_root: #not NaN\n",
        "        #|f(x)|\n",
        "        self.assertAlmostEqual(abs(f_root), 0, 8)\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 6.981s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQE6adceByNr",
        "colab_type": "code",
        "outputId": "bb052dd0-2432-4866-8879-f87f5661782d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "def vector_jacobian(f, x, dx=1e-6):\n",
        "  fx = f(x)\n",
        "  number_of_variables = fx[1]\n",
        "  J = np.zeros((fx[0].shape[0], number_of_variables), dtype='float64')\n",
        "  dxi = x.copy()\n",
        "\n",
        "  for i in range(x.shape[0]):\n",
        "    dxi[i] = dxi[i] + dx\n",
        "    fdxi = f(dxi)\n",
        "    J[:,i] = (fdxi[0]-fx[0])/dx \n",
        "    dxi[i] = x[i]\n",
        "  \n",
        "  return J\n",
        "\n",
        "def vector_newtons_method(f, x0, TOL = 1e-8):\n",
        "  x = x0\n",
        "\n",
        "  fx = f(x)\n",
        "\n",
        "  #nums_of_its = 0 \n",
        "\n",
        "  while np.linalg.norm(fx[0]) > TOL:\n",
        "    fx = f(x)\n",
        "    df = vector_jacobian(f,x)\n",
        "    dx = np.linalg.solve(df, -fx[0])\n",
        "    x = x + dx\n",
        "    #nums_of_its += 1\n",
        "\n",
        "  #print(\"Nums of its: \", nums_of_its)\n",
        "  return x\n",
        "\n",
        "class Test(unittest.TestCase):\n",
        "\n",
        "  def function1(x):\n",
        "    return (np.array([(x[0]**2)*x[1], 5*x[0]+math.sin(x[1])], dtype='float64'),2)\n",
        "\n",
        "  def function2(x):\n",
        "    return (np.array([x[0]*math.cos(x[1]), x[0]*math.sin(x[1])], dtype='float64'),2)\n",
        "\n",
        "  def function3(x):\n",
        "    return (np.array([5*x[1], 4*(x[0]**2)*2*math.sin(x[1]*x[2]), x[1]*x[2]], dtype='float64'),3)\n",
        "\n",
        "  #left preconditioning so system converges\n",
        "  def test_functions(self):\n",
        "    for n in range(1000):\n",
        "      x0_2_variables = np.random.rand(2)\n",
        "      x0_3_variables = np.random.rand(3)\n",
        "\n",
        "      root_f1_approx = vector_newtons_method(function1, x0_2_variables)\n",
        "      root_f2_approx = vector_newtons_method(function2, x0_2_variables)\n",
        "      root_f3_approx = vector_newtons_method(function3, x0_3_variables)\n",
        "\n",
        "      #||f(x)||\n",
        "      self.assertAlmostEqual(np.linalg.norm(function1(root_f1_approx)[0]), 0, 8)\n",
        "      self.assertAlmostEqual(np.linalg.norm(function2(root_f2_approx)[0]), 0, 8)\n",
        "      self.assertAlmostEqual(np.linalg.norm(function3(root_f3_approx)[0]), 0, 8)\n",
        "\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 1.186s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m",
        "colab_type": "text"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe",
        "colab_type": "text"
      },
      "source": [
        "All algorithms were implemented and tested with random data several times. The precision of the algorithms differs but generally the output had a precision of around 8 decimals.    "
      ]
    }
  ]
}