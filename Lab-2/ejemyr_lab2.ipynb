{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ejemyr_lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363-VT20/blob/ejemyr/Lab-2/ejemyr_lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7",
        "colab_type": "text"
      },
      "source": [
        "# **Lab 2: Matrix factorization**\n",
        "**Christoffer Ejemyr**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm",
        "colab_type": "text"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJipbXtnjrJZ",
        "colab_type": "text"
      },
      "source": [
        "This lab aims to implement efficient algorithms to both multiply and factorize matrices. The focus lies both in mathematical accuracy and computational cost.\n",
        "\n",
        "All methods were implemented in a satisfactory way and the accuracy was generally high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3",
        "colab_type": "text"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo",
        "colab_type": "text"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is distributed under a certain license. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab_type": "code",
        "outputId": "a8b10835-c325-4b91-88f8-4bcdc70c7177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2019 Christoffer Ejemyr (ejemyr@kth.se)\n",
        "# In collaboration with Leo Enge (leoe@kth.se)\n",
        "\n",
        "# This file is part of the course DD2363 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh",
        "colab_type": "text"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa",
        "colab_type": "text"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import unittest\n",
        "import numpy as np\n",
        "import scipy.sparse as sparse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJiVD0LvXN6e",
        "colab_type": "text"
      },
      "source": [
        "This lab aims to implement efficient algorithms to both multiply and factorize matrices. The focus lies both in mathematical accuracy and computational cost.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeFO9QMeUOAu",
        "colab_type": "text"
      },
      "source": [
        "# **Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7YR1kG_RYLl",
        "colab_type": "text"
      },
      "source": [
        "## Sparce matrix class\n",
        "This class saves and handles sparce matrices in CRS format. I've overvritten the numpy method *dot(self, other)* to manually define the matrix-vector product. The algorithm uses the liniarity of matrix-vector multiplication by adding the contribution from each non-zero element in the sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWsprvd7pCVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SparseMatrix(sparse.csr_matrix):\n",
        "    def dot(self, other):\n",
        "        if not(type(other) == np.ndarray and other.ndim == 1):\n",
        "            raise Exception(\"Vector format not recognized.\")\n",
        "        \n",
        "        if other.size != self.shape[1]:\n",
        "            raise Exception(\"Vector is of wrong length.\")\n",
        "\n",
        "        b = np.zeros(self.shape[0])\n",
        "        for i in range(self.shape[0]):\n",
        "            for j in range(self.indptr[i], self.indptr[i + 1]):\n",
        "                b[i] += self.data[j] * other[self.indices[j]]\n",
        "        return b\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.todense())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJtBIpbVPIHZ",
        "colab_type": "text"
      },
      "source": [
        "## QR-factorization\n",
        "The QR-factorization factorizes a matrix $A$ into a matrix $Q$ with normalized column vectors spanning $\\text{Range}(A)$ and an upper triangonal square matrix $R$ with values scaling the column vectors of $Q$ back to $A$.\n",
        "\n",
        "The algorithm used is the ordenary Gram-Schmidt method. Its simlisity makes it easely adaptable and is here implemented for all $m \\times n$ matrices where $m \\geq n$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daYOUpyXvOp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def QR_factorization(A):\n",
        "    if not(type(A) == np.ndarray and A.ndim == 2 and A.shape[1] <= A.shape[0]):\n",
        "        raise Exception(\"Matrix format not recognized.\")\n",
        "\n",
        "    R = np.zeros((A.shape[1], A.shape[1]))\n",
        "    Q = np.zeros(A.shape)\n",
        "    v = np.zeros(A.shape[0])\n",
        "    v[:] = A[:, 0]\n",
        "\n",
        "    for i in range(A.shape[1]):\n",
        "        R[i, i] = np.linalg.norm(v)\n",
        "        Q[:, i] = v / R[i, i]\n",
        "        for j in range(i + 1, A.shape[1]):\n",
        "            R[i, j] = Q[:, i].dot(A[:, j])\n",
        "\n",
        "        if i + 1 != A.shape[1]:\n",
        "            v[:] = A[:, i + 1]\n",
        "            for j in range(i + 1):\n",
        "                v[:] -= R[j, i + 1] * Q[:, j]\n",
        "    \n",
        "    return Q, R"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVCT6FJn9Cx3",
        "colab_type": "text"
      },
      "source": [
        "## Matrix equation solvers\n",
        "Below three functions for solving or optimizing matrix equations are implemented. All functions solve equations on the form\n",
        "$$ A x = b. $$\n",
        "The *backwards_substitution* function works for square, upper triangular matrices $A$. It then uses posibility to explicitly calculate each component of $x$ by backtracking from the equation of the last row in $Ax=b$.\n",
        "\n",
        "The *eq_sys_solver* function only takes square non-singular matrices. Using QR-factorization and the fact that for square matrices $Q^{-1} = Q^T$ you get\n",
        "\n",
        "\\begin{align}\n",
        "Ax&=b \\\\\n",
        "QRx&=b \\\\\n",
        "Rx&=Q^Tb\n",
        "\\end{align}\n",
        "\n",
        "Since $R$ is upper triangonal the system can be solved by backwards substitution.\n",
        "\n",
        "In a similar manner the *least_squares* uses the fact that the solution $x$ to $A^TAx = A^Tb$ will minimise $||Ax - b||$ and thus be the least square solution. Since $A^TA$ is a square matrix we can use the same method as in the *eq_sys_solver* function to solve this new matrix equation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_YfLjXmez8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backwards_substitution(U, b):\n",
        "    if not(type(U) == np.ndarray and U.ndim == 2):\n",
        "        raise Exception(\"Matrix format not recognized.\")\n",
        "    if (U != np.triu(U)).any():\n",
        "        raise Exception(\"Matrix is not upper triangular.\")\n",
        "    if np.linalg.det(U) == 0:\n",
        "        raise Exception(\"Matrix is singular\")\n",
        "    if not(type(b) == np.ndarray and b.ndim == 1):\n",
        "        raise Exception(\"Vector format not recognized.\")\n",
        "    if len(b) != U.shape[1]:\n",
        "        raise Exception(\"Vector and matrix formats does not match.\")\n",
        "    \n",
        "    n = U.shape[0]\n",
        "    x = np.zeros(n)\n",
        "    x[-1] = b[-1] / U[-1, -1]\n",
        "\n",
        "    for i in range(n - 2, -1, -1):\n",
        "        s = 0\n",
        "        for j in range(i + 1, n):\n",
        "            s += U[i, j] * x[j]\n",
        "        x[i] = (b[i] - s) / U[i, i]\n",
        "\n",
        "    return x\n",
        "\n",
        "def eq_sys_solver(A, b):\n",
        "    if A.shape[0] != A.shape[1]:\n",
        "        raise Exception(\"Matrix not square.\")\n",
        "    if np.linalg.det(A) == 0:\n",
        "        raise Exception(\"Matrix is singular.\")\n",
        "    if not(type(A) == np.ndarray and A.ndim == 2):\n",
        "        raise Exception(\"Matrix format not recognized.\")\n",
        "    if not(type(b) == np.ndarray and b.ndim == 1):\n",
        "        raise Exception(\"Vector format not recognized.\")\n",
        "    if len(b) != A.shape[0]:\n",
        "        raise Exception(\"Vector and matrix formats does not match.\")\n",
        "    \n",
        "    Q, R = QR_factorization(A)\n",
        "    return backwards_substitution(R, Q.transpose().dot(b))\n",
        "\n",
        "def least_squares(A, b):\n",
        "    if not(type(A) == np.ndarray and A.ndim == 2):\n",
        "        raise Exception(\"Matrix format not recognized.\")\n",
        "    if not(type(b) == np.ndarray and b.ndim == 1):\n",
        "        raise Exception(\"Vector format not recognized.\")\n",
        "    if len(b) != A.shape[0]:\n",
        "        raise Exception(\"Vector and matrix formats does not match.\")\n",
        "    \n",
        "    Q, R = QR_factorization(A.transpose().dot(A))\n",
        "    return backwards_substitution(R, Q.transpose().dot(A.transpose().dot(b)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF1Y_pnQD0u0",
        "colab_type": "text"
      },
      "source": [
        "## QR eigenvalue algorithm\n",
        "This eigenvalue algorithm will, after many itterations, converge A and U to the Schur factorization matrices. Since only square matrices will have eigenvalues we limit $A_{in}$ to beeing a square matrix. The method returns the approximation of the eigenvalues and the corresponding eigenvectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eE3gbc5ClA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eigen_vals_vecs(A_in, ittr :int):\n",
        "    A = A_in.copy()\n",
        "    if not(type(A) == np.ndarray and A.ndim == 2):\n",
        "        raise Exception(\"Matrix format not recognized.\")\n",
        "    if A.shape[0] != A.shape[1]:\n",
        "        raise Exception(\"Matrix not square.\")\n",
        "    U = np.eye(A.shape[0])\n",
        "    for i in range(ittr):\n",
        "        Q, R = QR_factorization(A)\n",
        "        A = R.dot(Q)\n",
        "        U = U.dot(Q)\n",
        "    return A.diagonal(), U"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc1nUGpYA2kG",
        "colab_type": "text"
      },
      "source": [
        "## Block matrix matrix multiplication\n",
        "My implementation divides the matrices into blocks using the folowing algorithm.\n",
        "\n",
        "Given a dimention of length $N$ that is to be divided into $n$ blocks the fisrt block will be of size\n",
        "$$d_1 = \\text{ceil}(N / n).$$\n",
        "The next block will then be of size\n",
        "$$d_2 = \\text{ceil}(\\frac{N - d_1}{n - 1}).$$\n",
        "Continuing with the $i$:th block beeing of size\n",
        "$$d_i = \\text{ceil}(\\frac{N - d_1 - d_2 - \\ldots - d_{i-1}}{n - (i - 1)}.)$$\n",
        "\n",
        "This method will garantuee that the blocks will be of sizes differing by maximally one element and that the larges blocks will be first follwed by the smaller blocks.\n",
        "\n",
        "I chose this method since it is very deterministic and not dependent on coinsidences between matrix sizes and bock numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5pySVVVBQaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def blocked_matrix_matrix(A, B, m :int, n :int, p :int):\n",
        "    if not(type(A) == np.ndarray and A.ndim == 2 and type(B) == np.ndarray and B.ndim == 2):\n",
        "        raise Exception(\"Matrix format not recognized.\")\n",
        "    if A.shape[1] != B.shape[0]:\n",
        "        raise Exception(\"Matrix format do not argree.\")\n",
        "    if m > A.shape[0] or m < 1 or n > B.shape[1] or n < 1 or p > A.shape[1] or p < 1:\n",
        "        raise Exception(\"Invlid number of blocks.\")\n",
        "\n",
        "    C = np.zeros((A.shape[0], B.shape[1]))\n",
        "\n",
        "    idx_i, idx_j, idx_k = 0, 0, 0\n",
        "    step_i, step_j, step_k = 0, 0, 0\n",
        "\n",
        "    for i in range(m):\n",
        "        idx_i += step_i\n",
        "        step_i = int(np.ceil((A.shape[0] - idx_i) / (m - i)))\n",
        "        idx_j = 0\n",
        "        step_j = 0\n",
        "        for j in range(n):\n",
        "            idx_j += step_j\n",
        "            step_j = int(np.ceil((B.shape[1] - idx_j) / (n - j)))\n",
        "            idx_k = 0\n",
        "            step_k = 0\n",
        "            for k in range(p):\n",
        "                idx_k += step_k\n",
        "                step_k = int(np.ceil((A.shape[1] - idx_k) / (p - k)))\n",
        "                C[idx_i : idx_i + step_i, idx_j : idx_j + step_j] += A[idx_i : idx_i + step_i, idx_k : idx_k + step_k].dot(B[idx_k : idx_k + step_k, idx_j : idx_j + step_j])\n",
        "                \n",
        "    return C"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cdBusjD2FO8",
        "colab_type": "text"
      },
      "source": [
        "# Tests\n",
        "Testing the algorithms mainly consists of two parts: checking raises and other assertions and testing for accuracy and floating point precition.\n",
        "\n",
        "Generally the first is done for some common mistakes and checks that exceptions are raised. The second test is done by multiple times generating random input data and testing either against nown results, like norm equal to zero, or against other algorithms that are known to be accurate.\n",
        "\n",
        "Most of the accurasy testing methods are strait forward and easy to understand. One that is more interesting is the method to test the least squares solution. Since the norm will not always be zero (only in special cases) we must instead check that the norm of the error is the smallest one for all x. What I did was to repeatidly add a small vector $v$ to the solution $x$ and check that the norm of the error was never smaller than the least squares solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rt2JT47al64",
        "colab_type": "code",
        "outputId": "5eade20c-6f2c-4559-cff6-dccffb18daec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "class TestSparseMatrix(unittest.TestCase):\n",
        "\n",
        "    def test_exceptions(self):\n",
        "        A = SparseMatrix([[1,0,0],[0,1,0],[0,0,1]])\n",
        "        B = SparseMatrix([[0,0,0],[0,0,0],[0,0,1]])\n",
        "        v = np.array([1, 4])\n",
        "        u = [1, 3, 4]\n",
        "        with self.assertRaises(Exception):\n",
        "            A.dot(B)\n",
        "        with self.assertRaises(Exception):\n",
        "            A.dot(v1)\n",
        "        with self.assertRaises(Exception):\n",
        "            A.dot(v2)\n",
        "\n",
        "    def test_accuracy(self):\n",
        "        max_n = 10\n",
        "        for i in range(num_of_tests):\n",
        "            n = np.random.randint(1, max_n)\n",
        "            M = np.random.rand(np.random.randint(1, max_n), n)\n",
        "            M_spase = SparseMatrix(M)\n",
        "            v = np.random.rand(n)\n",
        "            for i in range(7):\n",
        "                np.testing.assert_almost_equal(M.dot(v), M_spase.dot(v), decimal=i)\n",
        "\n",
        "\n",
        "class TestQRFactorization(unittest.TestCase):\n",
        "    def test_exceptions(self):\n",
        "        with self.assertRaises(Exception):\n",
        "            QR_factorization(np.array([1]))\n",
        "        with self.assertRaises(Exception):\n",
        "            QR_factorization([[1, 2], [3, 4]])\n",
        "            \n",
        "    def test_accuracy(self):\n",
        "        max_n = 10\n",
        "        for i in range(num_of_tests):\n",
        "            n = np.random.randint(1, max_n)\n",
        "            M = np.random.rand(n, n)\n",
        "\n",
        "            Q, R = QR_factorization(M)\n",
        "            M_reconstructed = Q.dot(R)\n",
        "\n",
        "            for i in range(7):\n",
        "                np.testing.assert_almost_equal(\n",
        "                    np.linalg.norm(Q.transpose().dot(Q)-np.eye(Q.shape[0]), 'fro'),\n",
        "                    0,\n",
        "                    decimal=i)\n",
        "                np.testing.assert_almost_equal(\n",
        "                    np.linalg.norm(Q.dot(R) - M, 'fro'),\n",
        "                    0,\n",
        "                    decimal=i)\n",
        "                np.testing.assert_almost_equal(R, np.triu(R), decimal=i)\n",
        "\n",
        "\n",
        "class TestBackwardsSub(unittest.TestCase):\n",
        "    def test_exceptions(self):\n",
        "        with self.assertRaises(Exception):\n",
        "            backwards_substitution(np.array([[1, 2, 3], [1, 2, 3], [0, 0, 1]]),\n",
        "                                  np.array([1, 2, 3]))\n",
        "            \n",
        "    def test_accuracy(self):\n",
        "        max_n = 10\n",
        "        for i in range(num_of_tests):\n",
        "            n = np.random.randint(1, max_n)\n",
        "            U = np.triu(np.random.rand(n, n))\n",
        "            x = np.random.rand(n)\n",
        "            b = U.dot(x)\n",
        "            for i in range(7):\n",
        "                np.testing.assert_almost_equal(x, backwards_substitution(U, b), decimal=i)\n",
        "\n",
        "\n",
        "class TestEqSolver(unittest.TestCase):\n",
        "    def test_accuracy(self):\n",
        "        max_n = 10\n",
        "        for i in range(num_of_tests):\n",
        "            n = np.random.randint(1, max_n)\n",
        "            A = np.random.rand(n, n)\n",
        "            x_true = np.random.rand(n)\n",
        "            b = A.dot(x_true)\n",
        "            x = eq_sys_solver(A, b)\n",
        "\n",
        "            for i in range(7):\n",
        "                np.testing.assert_almost_equal(\n",
        "                    np.linalg.norm(x - x_true),\n",
        "                    0,\n",
        "                    decimal=i)\n",
        "                np.testing.assert_almost_equal(\n",
        "                    np.linalg.norm(A.dot(x) - b),\n",
        "                    0,\n",
        "                    decimal=i)\n",
        "\n",
        "\n",
        "class TestLeastSquare(unittest.TestCase):\n",
        "    def test_accuracy(self):\n",
        "        max_dim = 10\n",
        "        for i in range(num_of_tests):\n",
        "            A = np.zeros((1,1))\n",
        "            while np.linalg.det(A.transpose().dot(A)) == 0:\n",
        "                m = np.random.randint(1, max_dim)\n",
        "                n = np.random.randint(1, m + 1)\n",
        "                A = np.random.rand(m, n)\n",
        "                b = np.random.rand(m)\n",
        "            x = least_squares(A, b)\n",
        "\n",
        "            for i in range(100):\n",
        "                diff_vec = 0.01 * (2 * np.random.rand(n) - 1)\n",
        "                self.assertTrue(np.linalg.norm(A.dot(x) - b) <= np.linalg.norm(A.dot(x + diff_vec) - b))\n",
        "\n",
        "\n",
        "class TestEigenValues(unittest.TestCase):\n",
        "    def test_accuracy(self):\n",
        "        max_n = 10\n",
        "        for i in range(num_of_tests):\n",
        "            n = np.random.randint(1, max_n)\n",
        "            A = np.random.rand(n, n)\n",
        "            A = A.transpose().dot(A)\n",
        "            eigen_vals, eigen_vectors = eigen_vals_vecs(A, 100)\n",
        "\n",
        "            for i in range(4):\n",
        "                for i, e in enumerate(eigen_vals):\n",
        "                    np.testing.assert_almost_equal(np.linalg.det(A - e * np.eye(A.shape[0])), 0, decimal=i)\n",
        "                    np.testing.assert_almost_equal(np.linalg.norm(A.dot(eigen_vectors[:, i]) - e * eigen_vectors[:, i]), 0, decimal=i)\n",
        "\n",
        "\n",
        "class TestBlockedMatrixMult(unittest.TestCase):\n",
        "    def test_accuracy(self):\n",
        "        max_dim = 50\n",
        "        for i in range(num_of_tests):\n",
        "            M = np.random.randint(1, max_dim + 1)\n",
        "            N = np.random.randint(1, max_dim + 1)\n",
        "            P = np.random.randint(1, max_dim + 1)\n",
        "            A = np.random.rand(M, P)\n",
        "            B = np.random.rand(P, N)\n",
        "\n",
        "            m = np.random.randint(1, M + 1)\n",
        "            n = np.random.randint(1, N + 1)\n",
        "            p = np.random.randint(1, P + 1)\n",
        "\n",
        "            for i in range(7):\n",
        "                np.testing.assert_almost_equal(\n",
        "                    blocked_matrix_matrix(A, B, m, n, p),\n",
        "                    A.dot(B),\n",
        "                    decimal=i\n",
        "                    )\n",
        "\n",
        "\n",
        "num_of_tests = 100\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "..........\n",
            "----------------------------------------------------------------------\n",
            "Ran 10 tests in 15.323s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_",
        "colab_type": "text"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "poXBrl37O0G2"
      },
      "source": [
        "All test passed with an accuracy of up to seven decimals, except the eigenvalue finder that only had an accuracy of someware around four decimal places. The accuracy of that method did increase with the number of itterations, but is very dependent on the matrix given."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m",
        "colab_type": "text"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe",
        "colab_type": "text"
      },
      "source": [
        "No suprices were encounterd and all methods implemented in a satisfactory way. I'm especially pleased with the block-size algorithm in the blocked matrix-matrix multiplication algorithm that I figured out on my own."
      ]
    }
  ]
}