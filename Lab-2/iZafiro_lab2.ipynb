{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iZafiro_lab2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7",
        "colab_type": "text"
      },
      "source": [
        "# **Lab 2: Matrix factorization**\n",
        "**Fabi치n Levic치n**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm",
        "colab_type": "text"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL",
        "colab_type": "text"
      },
      "source": [
        "This is the second lab in the course DD2363 Methods in Scientific Computing. It is about using Jupyter to implement three methods: one to calculate a sparse matrix-vector product, one to obtain the QR factorization of a matrix, and a direct solver of a linear system of equations. Some objectives may be to understand the benefits of sparse matrix representation, to become familiar with at least one QR factorization algorithm, and to understand how to use this algorithm to implement a direct solver. The results of these methods are then tested in various ways, and they are favorable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3",
        "colab_type": "text"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab_type": "code",
        "outputId": "357a5c65-b91a-47c6-f3cd-481eb427b1ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2020 Fabi치n Levic치n (fils2@kth.se)\n",
        "\n",
        "# This file is part of the course DD2363 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh",
        "colab_type": "text"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import math\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6",
        "colab_type": "text"
      },
      "source": [
        "In scientific computing one often encounters matrices with very few non-zero entries. These matrices are much more efficiently manipulated with a sparse matrix representation, which here consists of three vectors, than with the usual dense matrix representation. Here we implement a method to calculate a matrix-vector product given a matrix in such a representation.\n",
        "\n",
        "Any real square matrix $A$ can be factorized as $A = QR$, with $Q$ orthogonal, and $R$ upper triangular. There are various algorithms that may be used to obtain this factorization, including the Gram-Schmidt algorithm, Householder reflections, and Givens rotations. Each has its advantages and disadvantages. Here we implement the Gram-Schmidt algorithm, as the author was already familiar with it, and thus it was the simplest approach. \n",
        "\n",
        "To solve a linear system $Ax = b$ one may obtain the QR factorization of $A$ and use the property of orthogonal matrices that $Q^{-1} = Q^T$ to get $Rx = Q^Tb$, and then use backward substitution to solve for $x$. This is a direct solver, because intermediate steps don't approximate $x$, and is implemented here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeFO9QMeUOAu",
        "colab_type": "text"
      },
      "source": [
        "# **Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx",
        "colab_type": "text"
      },
      "source": [
        "A sparse matrix $A$ consists of three vectors:\n",
        "* $A[0]$, or *val*, which stores the entries of $A$ starting from the top left and moving from left to right, and then from top to bottom.\n",
        "* $A[1]$, or *col_idx*, where $A[1][i]$ stores the index of the column (in the dense matrix $A$) where $A[0][i]$ is supposed to go.\n",
        "* $A[2] (\\in \\mathbb{R}^{m + 1})$, or *row_ptr*, where $A[2][i]$ stores the index $j$ where $A[0][j]$ is the leftmost entry in the $i$-th row of the dense matrix $A$. $A[2][i] = A[2][i - 1]$ if all the entries in the $i$-th row of the dense matrix $A$ are $0$, $A[2][0] = 0$ if all the entries in the $0$-th row of the dense matrix $A$ are $0$, and $A[2][m] = len(A[0]) = len(val)$.\n",
        "\n",
        "The method sparseMatrixVectorProduct takes a sparse matrix $A$, a vector $x$, the number of rows ($m$) of the dense matrix $A$, and the number of columns ($n$) of the dense matrix $A$ as input, and returns the matrix-vector product $b = Ax$.\n",
        "\n",
        "The algorithm iterates ($i$) through the number of rows of the dense matrix $A$, takes the relevant Python slice of *col_idx*, iterates ($j$) through the indices of this slice, and adds $val[row\\_ptr[i] + j] \\cdot x[slice[j]]$ to the relevant entry of $b$. It is easy to verify that this is correct by definition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hcCaYY5Io3ph",
        "colab": {}
      },
      "source": [
        "def sparseMatrixVectorProduct(A, x, m, n):\n",
        "  val = A[0]\n",
        "  col_idx = A[1]\n",
        "  row_ptr = A[2]\n",
        "  b = np.zeros(m)\n",
        "\n",
        "  for i in range(m):\n",
        "    col_idx_slice = col_idx[row_ptr[i]:row_ptr[i + 1]]\n",
        "    for j in range(len(col_idx_slice)):\n",
        "      b[i] += val[row_ptr[i] + j]*x[col_idx_slice[j]]  \n",
        "  return b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzCjHtczfRPD",
        "colab_type": "text"
      },
      "source": [
        "The method qrFact takes a real square matrix $A \\in \\mathbb{R}^{n \\times n}$ as input, and returns a tuple $(Q, R)$ containing its QR factorization.\n",
        "\n",
        "The algorithm computes (in order), for every column $a_i$ of $A$, the vectors $Q[i] = a_i - \\sum_{j = 0}^{i - 1} proj_{Q[j]}a_i$, where $proj_{u}v$ for $u, v \\in \\mathbb{R}^n$ is the projection of $v$ on $u$ defined as $\\frac{\\langle u, v \\rangle}{\\langle u, u \\rangle}u$. The algorithm then normalizes these vectors and transposes the matrix $Q$. Finally, the upper triangular $R$ is obtained by noting that $R[i][j] = \\langle e_i, a_j \\rangle$, where $e_i$ is the $i$-th column of $Q$. This is the Gram-Schmidt algorithm. The author used [this](https://en.wikipedia.org/wiki/QR_decomposition) Wikipedia article as reference.\n",
        "\n",
        "Finally, note that $A$ is singular, then if the $i$-th column of $A$ is linearly dependent with the previous ones, then $Q[i]$ is the zero vector, and here the algorithm would raise an error. To avoid this, the algorithm simply removes $Q[i]$ from the $Q$ matrix (this can be done because we want to obtain an orthonormal basis for the column space of $A$). Here we have two options:\n",
        "\n",
        "* Output $Q \\in \\mathbb{R}^{n \\times dim(CS(A))}$, where $CS(A)$ is the column space of $A$, and thus output $R \\in \\mathbb{R}^{dim(CS(A)) \\times n}$.\n",
        "* Output $Q \\in \\mathbb{R}^{n^2}$, but artificially choose the remaining $n - dim(CS(A))$ orthonormal vectors.\n",
        "\n",
        "The choice is inconsequential (in either case $A = QR$), but author decided to go with the first option, as he thinks it is more important that $Q$ encodes an orthonormal basis of the column space of $A$ than that $Q$ and $R$ are square."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx4QM_1CTAsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def qrFact(A):\n",
        "  n = A.shape[0]\n",
        "  A = np.transpose(A)\n",
        "  Q = np.empty((n, n))\n",
        "\n",
        "  # Calculate the Q matrix\n",
        "  sum = np.empty(n)\n",
        "  zeroQis = list()\n",
        "  for i in range(n):\n",
        "    sum[:] = 0\n",
        "    for j in range(i):\n",
        "      if(j not in zeroQis):\n",
        "        sum += (Q[j] @ A[i])/(Q[j] @ Q[j])*Q[j]\n",
        "    Q[i] = A[i] - sum\n",
        "\n",
        "    # Take note of the indices i such that Q[i] is the zero vector\n",
        "    if(np.isclose(Q[i], np.zeros(n)).all()):\n",
        "      zeroQis.append(i)\n",
        "\n",
        "  # Normalize the non-zero Q[i]s\n",
        "  for i in range(n):\n",
        "    if(i not in zeroQis):\n",
        "      Q[i] = Q[i]/np.linalg.norm(Q[i])\n",
        "  \n",
        "  # Delete the zero Q[i]s\n",
        "  finalQ = list()\n",
        "  for i in range(n):\n",
        "    if(not np.isclose(Q[i], np.zeros(n)).all()):\n",
        "      finalQ.append(Q[i])\n",
        "  finalQ = np.array(finalQ)\n",
        "\n",
        "  # Calculate the R matrix\n",
        "  R = np.zeros((n, len(finalQ)))\n",
        "  for i in range(n):\n",
        "    for j in range(i + 1):\n",
        "      if(j < len(finalQ)):\n",
        "        R[i][j] = finalQ[j] @ A[i]\n",
        "\n",
        "  # Transpose both matrices\n",
        "  finalQ = np.transpose(finalQ)\n",
        "  R = np.transpose(R)\n",
        "\n",
        "  return (finalQ, R)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlBLeywniuS8",
        "colab_type": "text"
      },
      "source": [
        "The method directSolve takes a square matrix $A \\in \\mathbb{R}^{n \\times n}$, and a vector $b \\in \\mathbb{R}^n$ as input, and returns a vector $x \\in \\mathbb{R}^n$ such that $Ax = b$.\n",
        "\n",
        "To do this, we obtain the QR factorization of $A$ and use the property of orthogonal matrices that $Q^{-1} = Q^T$ to get $Rx = Q^Tb$, and then use the usual backward substitution to solve for $x$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YNeJ4p2TFSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def directSolve(A, b):\n",
        "  n = A.shape[0]\n",
        "  qrFactA = qrFact(A)\n",
        "  rhs = np.transpose(qrFactA[0]) @ b\n",
        "  R = qrFactA[1]\n",
        "  x = np.empty(n)\n",
        "\n",
        "  x[n - 1] = rhs[n - 1]/R[n - 1][n - 1]\n",
        "  for i in range(n - 2, -1, -1):\n",
        "    sum = 0\n",
        "    for j in range(i + 1, n):\n",
        "      sum += R[i][j]*x[j]\n",
        "    x[i] = (rhs[i] - sum)/R[i][i]\n",
        "\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_",
        "colab_type": "text"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_67Bde4zzz1I",
        "colab_type": "text"
      },
      "source": [
        "The method sparseMatrixVectorProduct is tested first with a matrix obtained from class, and then with two matrices obtained from [this](https://en.wikipedia.org/wiki/Sparse_matrix) Wikipedia article. Test $2$ also contains the second special case mentioned in the third point of the list in the description for the method.\n",
        "\n",
        "The tests consist simply of comparing the results of the sparse matrix-vector product and the dense matrix-vector product entry-by-entry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk77RAlQTN89",
        "colab_type": "code",
        "outputId": "d991dd10-eae4-4af7-c974-488e4c061204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# sparseMatrixVectorProduct tests\n",
        "\n",
        "# Test 1: Matrices A obtained from class\n",
        "\n",
        "val = [3, 2, 2, 2, 1, 1, 3, 2, 1, 2, 3]\n",
        "col_idx = [0, 1, 3, 1, 2, 2, 2, 3, 4, 4, 5]\n",
        "row_ptr = [0, 3, 5, 6, 8, 9, 11]\n",
        "ASparse = np.array([val, col_idx, row_ptr])\n",
        "x = np.array([1, 1, 1, 1, 1, 1])\n",
        "m = 6\n",
        "n = 6\n",
        "\n",
        "ADense = np.array([[3, 2, 0, 2, 0, 0],\n",
        "                  [0, 2, 1, 0, 0, 0],\n",
        "                  [0, 0, 1, 0, 0, 0],\n",
        "                  [0, 0, 3, 2, 0, 0],\n",
        "                  [0, 0, 0, 0, 1, 0],\n",
        "                  [0, 0, 0, 0, 2, 3]])\n",
        "\n",
        "assert((sparseMatrixVectorProduct(ASparse, x, m, n) == ADense @ x).all())\n",
        "\n",
        "# Test 2: Matrices A obtained from Wikipedia\n",
        "\n",
        "val = [5, 8, 3, 6]\n",
        "col_idx = [0, 1, 2, 1]\n",
        "row_ptr = [0, 0, 2, 3, 4]\n",
        "ASparse = np.array([val, col_idx, row_ptr])\n",
        "x = np.array([2, 3, 5, 7])\n",
        "m = 4\n",
        "n = 4\n",
        "\n",
        "ADense = np.array([[0, 0, 0, 0],\n",
        "                   [5, 8, 0, 0],\n",
        "                   [0, 0, 3, 0],\n",
        "                   [0, 6, 0, 0]])\n",
        "\n",
        "assert((sparseMatrixVectorProduct(ASparse, x, m, n) == ADense @ x).all())\n",
        "\n",
        "# Test 3: Matrices A obtained from Wikipedia\n",
        "\n",
        "val = [10, 20, 30, 40, 50, 60, 70, 80]\n",
        "col_idx = [0, 1, 1, 3, 2, 3, 4, 5]\n",
        "row_ptr = [0, 2, 4, 7, 8]\n",
        "ASparse = np.array([val, col_idx, row_ptr])\n",
        "x = np.array([-1, -0.5, 0, 0.5, 1, 1.5])\n",
        "\n",
        "ADense = np.array([[10, 20, 0, 0, 0, 0],\n",
        "                   [0, 30, 0, 40, 0, 0],\n",
        "                   [0, 0, 50, 60, 70, 0],\n",
        "                   [0, 0, 0, 0, 0, 80]])\n",
        "\n",
        "assert((sparseMatrixVectorProduct(ASparse, x, m, n) == ADense @ x).all())\n",
        "\n",
        "print(\"Tests passed successfully!\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests passed successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9mxUUl-sfcS",
        "colab_type": "text"
      },
      "source": [
        "The method qrFact is tested with four matrices $A$, two non-singular (see `# Test 1` and `# Test 4`), and two singular (see `# Test 2` and `# Test 3`). Each test consists of three assertions:\n",
        "\n",
        "* The $R$ matrix returned by the method (`qrFactA[1]`) is the same as its upper triangular part. In the case of $A$ singular, NumPy considers only the first $dim(CS(A))$ rows, as $R \\in \\mathbb{R}^{dim(CS(A)) \\times n}$.\n",
        "* The Frobenius norm $\\|Q^TQ - I_n\\|_F = 0$ within a tolerance. In the case of $A$ singular, $I_n = I_{dim(CS(A))}$ instead (it can be shown that $QQ^T$ is not necessarily equal to $I_n$, i.e., that $Q^T$ is a left inverse of $Q$, but not a right inverse).\n",
        "* The Frobenius norm $\\|QR - A\\|_F = 0$ within a tolerance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCDzgBgsjgiL",
        "colab_type": "code",
        "outputId": "1dc5ce38-5e2f-4745-876c-5fcb7f1a7ac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# qrFact tests\n",
        "\n",
        "# Test 1: Matrix A obtained from Wikipedia\n",
        "\n",
        "A1 = np.array([[12, -51, 4],\n",
        "               [6, 167, -68],\n",
        "               [-4, 24, -41]])\n",
        "\n",
        "qrFactA = qrFact(A1)\n",
        "\n",
        "# Check if R is upper triangular\n",
        "\n",
        "assert((qrFactA[1] == np.triu(qrFactA[1])).all())\n",
        "\n",
        "# Frobenius norm tests\n",
        "\n",
        "assert(np.linalg.norm(np.transpose(qrFactA[0]) @ qrFactA[0] - np.identity(A1.shape[0])) < 0.0001) #QTQ - I\n",
        "assert(np.linalg.norm(qrFactA[0] @ qrFactA[1] - A1) < 0.0001) #QR - A\n",
        "\n",
        "# Test 2: Singular matrix\n",
        "\n",
        "A2 = np.array([[1, 2, 3, 4],\n",
        "               [5, 6, 7, 8],\n",
        "               [-1, -2, -3, -4],\n",
        "               [0.2, 0.3, -0.2, -0.3]])\n",
        "\n",
        "qrFactA = qrFact(A2)\n",
        "\n",
        "# Check if R is upper triangular\n",
        "\n",
        "assert((qrFactA[1] == np.triu(qrFactA[1])).all())\n",
        "\n",
        "# Frobenius norm tests\n",
        "\n",
        "assert(np.linalg.norm(np.transpose(qrFactA[0]) @ qrFactA[0] - np.identity(qrFactA[0].shape[1])) < 0.0001) #QTQ - I\n",
        "assert(np.linalg.norm(qrFactA[0] @ qrFactA[1] - A2) < 0.0001) #QR - A\n",
        "\n",
        "# Test 3: Singular matrix\n",
        "\n",
        "A3 = np.array([[0, 1, 0, 1],\n",
        "               [1, 0, 1, 0],\n",
        "               [0, 1, 0, 1],\n",
        "               [1, 0, 1, 0]])\n",
        "\n",
        "qrFactA = qrFact(A3)\n",
        "\n",
        "# Check if R is upper triangular\n",
        "\n",
        "assert(np.allclose(qrFactA[1], np.triu(qrFactA[1])))\n",
        "\n",
        "# Frobenius norm tests\n",
        "\n",
        "assert(np.linalg.norm(np.transpose(qrFactA[0]) @ qrFactA[0] - np.identity(qrFactA[0].shape[1])) < 0.0001) #QTQ - I\n",
        "assert(np.linalg.norm(qrFactA[0] @ qrFactA[1] - A3) < 0.0001) #QR - A\n",
        "\n",
        "# Test 4: Non-singular matrix\n",
        "\n",
        "A4 = np.array([[0, 1, 0, 1],\n",
        "               [1, 0, 1, 0],\n",
        "               [0, 1, 1, 1],\n",
        "               [1, 1, 1, 0]])\n",
        "\n",
        "qrFactA = qrFact(A4)\n",
        "\n",
        "# Check if R is upper triangular\n",
        "\n",
        "assert((qrFactA[1] == np.triu(qrFactA[1])).all())\n",
        "\n",
        "# Frobenius norm tests\n",
        "\n",
        "assert(np.linalg.norm(np.transpose(qrFactA[0]) @ qrFactA[0] - np.identity(A4.shape[0])) < 0.0001) #QTQ - I\n",
        "assert(np.linalg.norm(qrFactA[0] @ qrFactA[1] - A4) < 0.0001) #QR - A\n",
        "\n",
        "print(\"Tests passed successfully!\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests passed successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaFZuPUb1B2p",
        "colab_type": "text"
      },
      "source": [
        "The method directSolve is tested by solving the linear equations $Ax = b$ where the $A$s are the non-singular matrices defined in the tests for the previous method. The method is tested with two $b$ vectors for each $A$. Each test consists of two assertions:\n",
        "\n",
        "* The residual $\\|Ax - b\\| = 0$ within a tolerance.\n",
        "* The Euclidian norm $\\|x - y\\| = 0$ within a tolerance, where $y$ is NumPy's solution to the associated linear equation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79w1rrrgjhGN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97b68b52-e4ed-4930-c0e6-f248064e8b0e"
      },
      "source": [
        "# directSolve tests\n",
        "\n",
        "# Test 1\n",
        "\n",
        "b1 = [1, 1, 1]\n",
        "\n",
        "# norm(Ax - b) test\n",
        "\n",
        "assert(np.linalg.norm(A1 @ directSolve(A1, b1) - b1) < 0.0001)\n",
        "\n",
        "# norm(x - y) test\n",
        "\n",
        "assert(np.linalg.norm(directSolve(A1, b1) - np.linalg.solve(A1, b1)) < 0.0001)\n",
        "\n",
        "# Test 2\n",
        "\n",
        "b2 = [-1, 0, 1]\n",
        "\n",
        "# norm(Ax - b) test\n",
        "\n",
        "assert(np.linalg.norm(A1 @ directSolve(A1, b2) - b2) < 0.0001)\n",
        "\n",
        "# norm(x - y) test\n",
        "\n",
        "assert(np.linalg.norm(directSolve(A1, b2) - np.linalg.solve(A1, b2)) < 0.0001)\n",
        "\n",
        "# Test 3\n",
        "\n",
        "b3 = [0.2, 0.3, -0.2, -0.3]\n",
        "\n",
        "# norm(Ax - b) test\n",
        "\n",
        "assert(np.linalg.norm(A4 @ directSolve(A4, b3) - b3) < 0.0001)\n",
        "\n",
        "# norm(x - y) test\n",
        "\n",
        "assert(np.linalg.norm(directSolve(A4, b3) - np.linalg.solve(A4, b3)) < 0.0001)\n",
        "\n",
        "# Test 4\n",
        "\n",
        "b4 = [-1, 0, 1, 2]\n",
        "\n",
        "# norm(Ax - b) test\n",
        "\n",
        "assert(np.linalg.norm(A4 @ directSolve(A4, b4) - b4) < 0.0001)\n",
        "\n",
        "# norm(x - y) test\n",
        "\n",
        "assert(np.linalg.norm(directSolve(A4, b4) - np.linalg.solve(A4, b4)) < 0.0001)\n",
        "\n",
        "print(\"Tests passed successfully!\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests passed successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m",
        "colab_type": "text"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe",
        "colab_type": "text"
      },
      "source": [
        "All the results were favorable.\n",
        "\n",
        "The author spent most of his time dealing with the singular case in the Gram-Schmidt algorithm for QR factorization, and revisited many elementary facts from linear algebra. His conclusion about this is that the theoretical aspects of these algorithms, even if sometimes ill-motivated or arbitrary from an applied perspective, can have important practical implications, especially while dealing with edge cases.\n",
        "\n",
        "The author also appreciates how easy it is to turn a QR factorization algorithm into a direct solver for linear equations."
      ]
    }
  ]
}