{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vonPlaten-Edvin-report-lab-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7",
        "colab_type": "text"
      },
      "source": [
        "# **Lab 2: Matrix Factorization**\n",
        "**Edvin von Platen**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm",
        "colab_type": "text"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJipbXtnjrJZ",
        "colab_type": "text"
      },
      "source": [
        "In this lab we implement, test, and evaluate the following matrix algorithms:\n",
        "\n",
        "1. Sparse matrix-vector product $b = Ax$,\n",
        "2. Classical Gram-Schmidt QR Factorization $A=QR$,\n",
        "3. A Direct Solver using Backsubstitution and QR Factorization $Ax = b \\iff QRx = b$,\n",
        "4. Least Squares problem $Ax = b$. \n",
        "\n",
        "Our implementation of the algorithms appear to be sound and they perform quite well when compared to the implementations found in the numpy library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3",
        "colab_type": "text"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab_type": "code",
        "outputId": "f74fa781-413b-41e7-a2ec-1bba2288ad4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2019 Edvin von Platen (edvinvp@kth.se)\n",
        "\n",
        "# This file is part of the course DD2363 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh",
        "colab_type": "text"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa",
        "colab_type": "text"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import unittest\n",
        "import random\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "np.random.seed(seed=1994)\n",
        "random.seed(1994)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6",
        "colab_type": "text"
      },
      "source": [
        "We implement and evaluate the following matrix algorithms:\n",
        "\n",
        "1. Sparse matrix-vector product $b = Ax$,\n",
        "2. Classical Gram-Schmidt QR Factorization $A=QR$,\n",
        "3. A Direct Solver using Backsubstitution and QR Factorization $Ax = b \\iff QRx = b$,\n",
        "4. Least Squares problem $Ax = b$.\n",
        "\n",
        "All implementations and mathematical conccepts presented in this report are based on the lecture notes from the course [DD2363 Methods in Scientific Computing](https://kth.instructure.com/courses/17068). \n",
        "\n",
        "In the Methods section we present background and implementation for each algrithm, including unittest implementation. Followed by test results in the Results section and a brief discussion of the results in the Discussion section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeFO9QMeUOAu",
        "colab_type": "text"
      },
      "source": [
        "# **Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx",
        "colab_type": "text"
      },
      "source": [
        "## **Sparse Matrix-Vector Product**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMzJV5vQUP9b",
        "colab_type": "text"
      },
      "source": [
        "We implement Algorithm 5.9. sparse matrix-vector product. Input is a  matrix $A \\in R^{m\\times n}$ represented in compressed row storage (CRS) format, a vector $x \\in R^n$, and output is the vector $b = Ax, \\ b \\in R^m$.\n",
        "\n",
        "A matrix in the CRS format is represented through three arrays: *val*, *col_idx*, and *row_ptr*. val stores the nonzero matrix values, col_idx the nonzero values column indices, and row_ptr the indices of the of the first nonzero values of each row in val plus the number of vals.\n",
        "\n",
        "For example, given the matrix $$C = \\begin{pmatrix} 0 & 0 & 1 & 0 \\\\ 2 & 0 & 3 &0 \\\\ 0 & 0 & 0 & 0  \\\\ 0 & 5 & 1 & 0 \\end{pmatrix} $$\n",
        "the three arrays are\n",
        "\\begin{align*}\n",
        "val &= [1,2,3,5,1] \\\\\n",
        "col\\_idx &= [2,0,2,1,2] \\\\\n",
        "row\\_idx &= [0,1,3,3,5].\n",
        "\\end{align*}\n",
        "When there is a zero row the next row index is repeated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7At5PloY4kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using the CRS matrix class from the appendix\n",
        "# Construct a simple sparse matrix class using the CRS data structure\n",
        "class spMatrix:\n",
        "\n",
        "  def __init__(self, m, A = None, val = None, col_idx = None, row_idx = None):\n",
        "    if A is None:\n",
        "      self.val = val\n",
        "      self.col_idx = col_idx\n",
        "      self.row_idx = row_idx\n",
        "      # Had to include m to allow for zero rows in matrix-vector multiplication\n",
        "      self.m = m\n",
        "    else:\n",
        "      # Make A sparse\n",
        "      non_zeros = np.count_nonzero(A)\n",
        "      v = np.zeros(non_zeros)\n",
        "      r = []\n",
        "      non_zero_row, non_zero_col = np.nonzero(A)\n",
        "      prev_row = -1\n",
        "      for i in range(len(non_zero_row)):\n",
        "        v[i] = A[non_zero_row[i], non_zero_col[i]]\n",
        "        # Check if we have skipped a row\n",
        "        if (prev_row == non_zero_row[i] - 1):\n",
        "          # no row was skipped\n",
        "          prev_row = non_zero_row[i]\n",
        "          r.append(i)\n",
        "        elif (prev_row == non_zero_row[i]):\n",
        "          # Still on same row do nothing\n",
        "          pass\n",
        "        else:\n",
        "          # atleast one row was skipped fill with current idx\n",
        "          for j in range(prev_row, non_zero_row[i]):\n",
        "            r.append(i)\n",
        "          prev_row = non_zero_row[i]\n",
        "\n",
        "      r.append(len(v))\n",
        "      self.val = v\n",
        "      self.col_idx = non_zero_col\n",
        "      self.row_idx = np.array(r)\n",
        "      self.m = m\n",
        "\n",
        "  def print_components(self):\n",
        "    print(\"val = \" + str(self.val))\n",
        "    print(\"col_idx = \" + str(self.col_idx))\n",
        "    print(\"row_idx = \" + str(self.row_idx))\n",
        "\n",
        "def sparse_matrix_vector_product(A, x):\n",
        "  k = len(A.row_idx) - 1\n",
        "  b = np.zeros((A.m,1))\n",
        "  for i in range(k):\n",
        "    for j in range(A.row_idx[i], (A.row_idx[i+1])):\n",
        "      b[i,0] += A.val[j]*x[A.col_idx[j]]\n",
        "  return b\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwtWaG0hOLv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tests for sparse matrix-vector product\n",
        "class Sparse_matrix_vector_test(unittest.TestCase):\n",
        "  def test_zero(self):\n",
        "    A = np.array([[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]])\n",
        "    b = np.array([[0],[1],[2],[3]])\n",
        "    spA = spMatrix(4, A=A)\n",
        "    b1 = sparse_matrix_vector_product(spA,b)\n",
        "    b2 = A.dot(b)\n",
        "    self.assertIs(np.allclose(b1,b2, atol=1e-05), True)\n",
        "\n",
        "  def test_random_dense(self):\n",
        "    # run 100 test with different dimensions\n",
        "    for i in range(10):\n",
        "      m = random.randint(1,50)\n",
        "      n = random.randint(1,50)\n",
        "      A = 1000 * np.random.random_sample((m, n)) - 500 # [-500,500]\n",
        "      b = 1000 * np.random.random_sample((n,1)) - 500\n",
        "      spA = spMatrix(m,A=A)\n",
        "      self.assertIs(np.allclose(sparse_matrix_vector_product(spA,b), A.dot(b), atol=1e-04), True)\n",
        "\n",
        "  def test_random_sparse(self):\n",
        "    m = random.randint(50,200)\n",
        "    n = random.randint(50,200)\n",
        "    A = np.random.randint(0,10, m * n)\n",
        "    # make around 10% of the array non-zero\n",
        "    A[A > 1] = 0\n",
        "    A = np.reshape(A, (m,n))\n",
        "    b = np.reshape(np.random.randint(0,10,n), (n,1))\n",
        "    spA = spMatrix(m, A=A)\n",
        "    self.assertIs(np.allclose(sparse_matrix_vector_product(spA,b), A.dot(b), atol=1e-04), True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sev5dkyhWrD",
        "colab_type": "text"
      },
      "source": [
        "## **Gram-Schmidt QR Factorization**\n",
        "Given a non-singular matrix $A\\in R^{n\\times n}$, its $QR$ factorization is $A=QR$, where $Q$ is an orthogonal matrix and $R$ a upper triangular matrix.\n",
        "\n",
        "We implement the classical Gram-Schmidt QR iteration. The idea is that since $A$ is non-singular, we can always construct an orthonormal basis $\\{q_i\\}_{i=1}^j$ such that $<q_1,..,q_j> = <a_{:1},...,a_{:j}>, \\ \\forall j \\leq n$, i.e. $\\{q_i\\}_{i=1}^j$ and $\\{a_{:i}\\}_{i=j}^n$ span the same vector space. \n",
        "\n",
        "The orthonormal basis $\\{q_i\\}_{i=1}^n$ is constructed iteratively by equation (5.4),\n",
        "\\begin{align*}\n",
        "v_j &= a_{:j} - \\sum_{i=1}^{j-1} (a_{:j}, q_{j})q_i, \\\\\n",
        "q_j &= \\frac{v_j}{\\Vert v_j \\Vert}.\n",
        "\\end{align*}\n",
        "By rewriting (5.4) as,\n",
        "\\begin{align*}\n",
        "a_{:1} &= r_{11}q_1 \\\\\n",
        "a_{:2} &= r_{12}q_1 + r_{22}q_2 \\\\\n",
        "       &\\vdots \\\\\n",
        "a_{:n} &= r_{1n}q_1 + \\dots + r_{nn}q_{n}\n",
        "\\end{align*}\n",
        "where $r_{ij} = (a_{:j},q_i)$ and $r_{ii} = \\Vert v_j \\Vert$. Which in matrix form is the $QR$ factorization $A = QR$,\n",
        "\n",
        "\\begin{align*}\n",
        "\\left( \\begin{array}{c|c|c|c} & & &  \\\\ a_{:1} & a_{:2} & \\dots & a_{:n} \\\\ & & & \\end{array} \\right) = \\left( \\begin{array}{c|c|c|c} & & &  \\\\ q_{1} & q_{2} & \\dots & q_{n} \\\\ & & & \\end{array} \\right)\\left( \\begin{array}{cccc} r_{11} & r_{12} & \\dots & r_{1n} \\\\  & r_{22} &  &  \\\\ \\vdots & & \\ddots & \\vdots \\\\ 0 & \\dots & & r_{nn}\\end{array} \\right)\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTkO6oVor52W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classic_gram_schmidt_qr_iteration(A):\n",
        "  n = A.shape[0]\n",
        "  Q = np.zeros((n,n))\n",
        "  R = np.zeros((n,n))\n",
        "  for j in range(n):\n",
        "    a_j = A[:,j]\n",
        "    s = 0\n",
        "    for i in range(j):\n",
        "      q_i = Q[:,i]\n",
        "      R[i, j] = q_i.dot(a_j)\n",
        "      s += R[i,j] * q_i\n",
        "    v_j = a_j - s\n",
        "    R[j,j] = np.linalg.norm(v_j)\n",
        "    q_j = v_j / R[j,j]\n",
        "    Q[:,j] = q_j\n",
        "  return Q,R"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3ZpVB5ZgNoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tests for classical gram-schmidt iteration\n",
        "class classic_gram_schmidt_qr_iteration_test(unittest.TestCase):\n",
        "  def test_random(self):\n",
        "    # run 100 test with different dimensions\n",
        "    for i in range(20):\n",
        "      m = random.randint(1,25)\n",
        "      A = 1000 * np.random.random_sample((m, m)) - 500 # [-500,500]\n",
        "      Q1,R1 = classic_gram_schmidt_qr_iteration(A)\n",
        "      Q2,R2 = np.linalg.qr(A)\n",
        "      # np.linalg.qr has different signs so we test by checking that QR = A\n",
        "      A1 = Q1.dot(R1)\n",
        "      A2 = Q2.dot(R2)\n",
        "      self.assertIs(np.allclose(A1,A2,atol=1e-05), True)\n",
        "      \n",
        "  def test_upper_tri(self):\n",
        "    # run 20 test with different dimensions\n",
        "    for i in range(20):\n",
        "      m = random.randint(1,25)\n",
        "      A = 1000 * np.random.random_sample((m, m)) - 500 # [-500,500]\n",
        "      Q1,R1 = classic_gram_schmidt_qr_iteration(A)\n",
        "      self.assertIs(np.allclose(R1, np.triu(R1), atol=1e-05), True)\n",
        "\n",
        "  def test_identity(self):\n",
        "    # run 20 test with different dimensions\n",
        "    for i in range(20):\n",
        "      m = random.randint(1,25)\n",
        "      A = 1000 * np.random.random_sample((m, m)) - 500 # [-500,500]\n",
        "      Q1,R1 = classic_gram_schmidt_qr_iteration(A)\n",
        "      I = Q1.dot(np.transpose(Q1))\n",
        "      self.assertIs(np.allclose(I, np.identity(m), atol=1e-05), True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsmNdio708EB",
        "colab_type": "text"
      },
      "source": [
        "## **Direct Solver**\n",
        "\n",
        "We implement a direct solver of the matrix equation $Ax = b, \\ A\\in R^{n\\times n}, \\ b \\in R^n, \\ x \\in R^n$, where $A$ is non-singular. Input is $A$ and $b$ and output $A^{-1} b = x$.\n",
        "\n",
        "The idea is to use the Gram-Schmidt QR factorization $A = QR$ of the previous section, we then have to solve $QRx = b$. The inverse of an orthogonal matrix is its transpose which gives us $Rx = Q^Tb$, which we can solve using *backward substitution*. \n",
        "\n",
        "Given an upper triangular $U \\in R^{n\\times n}$ matrix we can solve the equation $Ux = b$ using backward substitution. Note that $U_{n,n}x_n = b_n \\iff x_n = \\frac{b_n}{U_{n,n}}$, we can then substitue $x_n$ into $U_{n-1,n-1}x_{n-1} + U_{n-1, n}x_n = b_{n-1}$, which gives us,\n",
        "$$\n",
        "x_{n-1} = \\frac{b_{n-1} - U_{n-1,n}\\frac{b_n}{U_{n,n}}}{U_{n-1,n-1}}.\n",
        "$$\n",
        "And the formula,\n",
        "$$\n",
        "x_i = \\frac{b_i - \\sum_{j=i+1}^n U_{i,j}x_j}{U_{i,i}}\n",
        "$$\n",
        "\n",
        "This gives us Algorithm 5.2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPAwCY_S8Ebm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_substitution(U,b):\n",
        "  n = len(b)\n",
        "  x = np.zeros((n,1))\n",
        "  for i in range(n-1, -1, -1):\n",
        "    s = 0\n",
        "    for j in range(i+1, n):\n",
        "      s += U[i,j]*x[j,0]\n",
        "    x[i,0] = (b[i] - s) / U[i,i]\n",
        "  return x\n",
        "\n",
        "def transpose(A):\n",
        "  m,n = A.shape\n",
        "  A_T = np.zeros((n,m))\n",
        "  for i in range(n):\n",
        "    A_T[i,:] = A[:,i]\n",
        "  return A_T\n",
        "\n",
        "def direct_solver(A,b):\n",
        "  Q,R = classic_gram_schmidt_qr_iteration(A)\n",
        "  Q_T = transpose(Q)\n",
        "  Q_Tb = Q_T.dot(b)\n",
        "  x = backward_substitution(R, Q_Tb)\n",
        "  return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7gd5mFil6Bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tests for direct solver\n",
        "class direct_solver_test(unittest.TestCase):\n",
        "  def test_random(self):\n",
        "    # run 20 test with different dimensions\n",
        "    for i in range(20):\n",
        "      m = random.randint(1,25)\n",
        "      A = 1000 * np.random.random_sample((m, m)) - 500 # [-500,500]\n",
        "      b = 10 * np.random.random_sample((m,1)) - 5\n",
        "      x1 = direct_solver(A,b)\n",
        "      x2 = np.linalg.solve(A,b)\n",
        "      self.assertIs(np.allclose(x1,x2,atol=1e-05), True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61lIVvudUlCH",
        "colab_type": "text"
      },
      "source": [
        "## **Least Squares Problem**\n",
        "\n",
        "For a system of equations,\n",
        "\\begin{align}\n",
        "Ax = b,\n",
        "\\end{align}\n",
        "when $ A\\in R^{m\\times n}, \\ x \\in R^n, \\ b \\in R^m, m > n$ the system is said to be overdecided, i.e. there are more equations then unknowns. Finding a solution to this system is known as a least squares problem. \n",
        "\n",
        "An overview of the least squares problem is given in Example 2.14 which we will also give here with some added explanations. Assume that $rank(A^T) = n$, i.e. there are $n$ linearly independent row vectors in $A$. Then, by the fundamental theorem of linear algebra (Theorem 2.2), $n = rank(A^T) + dim(null(A)) \\implies null(A) = \\emptyset$. The idea is now to project $b$ onto the range of $A$ using an orthogonal projector $P$, i.e. the division of $R^n$ into the two subspaces $range(P)$ and $range(I-P)$ are orthogonal (Section 2.7). This means that $Pb \\in range(A)$ with the projection error $b-Pb\\in range(A)^\\perp$. \n",
        "\n",
        "Now, $Pb \\in range(A)$ means that there is some $\\bar{x}\\in R^n$ such that $Pb = A\\bar{x}$. This means that the residual of the system of equations is equal to the projection error, $b-Pb = b-A\\bar{x}$. Continue by noting that $range(A)^\\perp = null(A^T)$, we can see this by observing that we can view the matrix vector multiplication $A^Tx$ as a series of scalar products, \n",
        "$$\n",
        "A^T x = \\begin{pmatrix} a_{:1}\\cdot x \\\\ \\vdots \\\\ a_{:m} \\cdot x \\end{pmatrix}.\n",
        "$$\n",
        "Which equals the zero vector only if $x$ orthogonal to all column vectors of $A$. Thus, $A^T(b-A\\bar{x}) = 0 \\iff A^TA\\bar{x} = A^T b$, by Theorem 2.2, this condition is known as the normal equations (2.17). Solving the normal equations is thus the same as finding the best approximation of $b \\in R^m$ in $R^n$, i.e. find $\\bar{x} \\in R^n$ such that\n",
        "$$\n",
        "\\Vert b - A\\bar{x} \\Vert \\leq \\Vert b - Ay \\Vert, \\ \\forall y \\in R^n.\n",
        "$$\n",
        "Because,\n",
        "\\begin{align}\n",
        "\\Vert Ay - b \\Vert^2 &= \\Vert A(y-\\bar{x}) + (A\\bar{x} - b) \\Vert^2 \\ \\ \\ \\ \\ \\ \n",
        "(\\pm A\\bar{x}) \\\\\n",
        "&= \\Vert A(y-\\bar{x})\\Vert^2 + 2(A(y-\\bar{x}))^T(A\\bar{x} - b) + \\Vert A\\bar{x} - b\\Vert^2 \\\\\n",
        " & \\{ \\text{recall that } \\Vert (a + d) \\Vert^2 = (a,d)\\} \\\\\n",
        "& \\geq 2(y-\\bar{x})^TA^T(A\\bar{x} - b) + \\Vert A\\bar{x} - b\\Vert^2 = \\Vert A\\bar{x}- b\\Vert^2,\n",
        "\\end{align}\n",
        "as $A^T(A\\bar{x} - b) = 0$. Solving (2.17) for $\\bar{x}$ gives us,\n",
        "$$\n",
        "\\bar{x} = (A^T A)^{-1}A^Tb.\n",
        "$$\n",
        "We now give an implementation for solving a given least squares problem using the previously implemented classical Gram-Schmidt iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2ZisUgYuYv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def least_squares(A,b):\n",
        "  A_T = transpose(A)\n",
        "  A_TA = A_T.dot(A)\n",
        "  A_Tb = A_T.dot(b)\n",
        "  Q, R = classic_gram_schmidt_qr_iteration(A_TA)\n",
        "  Q_T = transpose(Q)\n",
        "  Q_TA_Tb = Q_T.dot(A_Tb)\n",
        "  x = backward_substitution(R, Q_TA_Tb)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2za44EhDnjET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tests for Least Squares\n",
        "class least_squares_test(unittest.TestCase):\n",
        "  def test_random(self):\n",
        "    # run 20 test with different dimensions\n",
        "    for i in range(20):\n",
        "      m = random.randint(5,10)\n",
        "      n = random.randint(1, m)\n",
        "      A = 1000 * np.random.random_sample((m, n)) - 500 # [-500,500]\n",
        "      b = np.random.random_sample((m,1))\n",
        "      x1 = least_squares(A,b)\n",
        "      x2 = np.linalg.solve(np.transpose(A).dot(A),np.transpose(A).dot(b))\n",
        "      self.assertIs(np.allclose(x1,x2,atol=1e-05), True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_",
        "colab_type": "text"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd",
        "colab_type": "text"
      },
      "source": [
        "In this section we will present the results of the tests for the implemented algorithms.\n",
        "\n",
        "First we evaulate all unittests in the previous section which cover, I was inspired to use unittests after reviewing Kristoffer Almroths lab 1 report.\n",
        "\n",
        "- **sparse_matrix_vector_multiplication(A,b)**: Test performace against np.dot() for both sparse- and dense matrices. Test behaviour when multiplying with all zero matrix.\n",
        "- **classical_gram_schmidt_qr_factorization(A)**: Test against np.lingalg.qr() for randomized matrices. Test that $R$ is upper triangular, test that $Q$ is orthogonal.\n",
        "- **direct_solver(A,b)**: Test against np.linalg.solve() for randomized matrices.\n",
        "- **least_squares(A,b)**: Test against np.linalg.solve() for the normal equastions calculated using np functions for randomized matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDcFrnOCpIf1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0e65e9ff-a7b9-43e0-f07d-4e211d900e39"
      },
      "source": [
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "........\n",
            "----------------------------------------------------------------------\n",
            "Ran 8 tests in 0.173s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f330c4e3ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMM5C2RarQC5",
        "colab_type": "text"
      },
      "source": [
        "All tests pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ubQcEUgu5iH",
        "colab_type": "text"
      },
      "source": [
        "#### **Gram-Schmidt QR Iteration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuC-slDLrUEi",
        "colab_type": "text"
      },
      "source": [
        "We continue by evaluating the Frobenius norms $\\Vert Q^TQ - I\\Vert_F, \\Vert Q R-A\\Vert_F$ for the classical Gram-Schmidt QR iteration implementation. The Frobenious norm is defined for a $m\\times n$ matrix $A$ as,\n",
        "$$\n",
        "\\Vert A \\Vert_F = trace(A^TA) = \\left( \\sum_{i=1}^m\\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2} \\ \\ \\ (section \\ 2.5).\n",
        "$$\n",
        "The Frobenious norm is the default norm in the np.linalg.norm() function. We evaluate the two Frobenious norms for 20 randomized matrices. We also compare the result with the np.linalg.qr() function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7-clk7ztRjR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86e8182a-8c25-4dc2-e866-1e6f49ee2a70"
      },
      "source": [
        "for i in range(10):\n",
        "  m = random.randint(1,100)\n",
        "  A = 1000 * np.random.random_sample((m, m)) - 500 # [-500,500]\n",
        "  Q,R = classic_gram_schmidt_qr_iteration(A)\n",
        "  Q2,R2 = np.linalg.qr(A)\n",
        "  print(\"A dim: \" + str(m) + \" times \" + str(m))\n",
        "  print(\"REPORT IMPLEMENTATION\")\n",
        "  print(\"||Q^TQ-I||_F = \" + str(np.linalg.norm(np.transpose(Q).dot(Q) - np.identity(m))))\n",
        "  print(\"||QR - A||_F =\" + str(np.linalg.norm(Q.dot(R) - A))) \n",
        "  print(\"USING np.linalg.qr()\")\n",
        "  print(\"||Q^TQ-I||_F = \" + str(np.linalg.norm(np.transpose(Q2).dot(Q2) - np.identity(m))))\n",
        "  print(\"||QR - A||_F =\" + str(np.linalg.norm(Q2.dot(R2) - A))) "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A dim: 14 times 14\n",
            "REPORT IMPLEMENTATION\n",
            "||Q^TQ-I||_F = 1.7268732252062096e-14\n",
            "||QR - A||_F =3.2241029558033507e-13\n",
            "USING np.linalg.qr()\n",
            "||Q^TQ-I||_F = 2.148927943038132e-15\n",
            "||QR - A||_F =1.8364837177524834e-12\n",
            "A dim: 60 times 60\n",
            "REPORT IMPLEMENTATION\n",
            "||Q^TQ-I||_F = 5.754687946844909e-13\n",
            "||QR - A||_F =2.2608797146494282e-12\n",
            "USING np.linalg.qr()\n",
            "||Q^TQ-I||_F = 5.4463290832238415e-15\n",
            "||QR - A||_F =9.225592823923896e-12\n",
            "A dim: 61 times 61\n",
            "REPORT IMPLEMENTATION\n",
            "||Q^TQ-I||_F = 9.169919903451057e-14\n",
            "||QR - A||_F =2.4243252002880867e-12\n",
            "USING np.linalg.qr()\n",
            "||Q^TQ-I||_F = 5.12419973155947e-15\n",
            "||QR - A||_F =9.327007245872977e-12\n",
            "A dim: 50 times 50\n",
            "REPORT IMPLEMENTATION\n",
            "||Q^TQ-I||_F = 6.064871935216519e-14\n",
            "||QR - A||_F =1.8980225389607257e-12\n",
            "USING np.linalg.qr()\n",
            "||Q^TQ-I||_F = 4.46409627713528e-15\n",
            "||QR - A||_F =7.016707848843544e-12\n",
            "A dim: 87 times 87\n",
            "REPORT IMPLEMENTATION\n",
            "||Q^TQ-I||_F = 3.472236634550275e-13\n",
            "||QR - A||_F =3.753021350021316e-12\n",
            "USING np.linalg.qr()\n",
            "||Q^TQ-I||_F = 7.241937092625537e-15\n",
            "||QR - A||_F =1.5270424516674464e-11\n",
            "A dim: 89 times 89\n",
            "REPORT IMPLEMENTATION\n",
            "||Q^TQ-I||_F = 1.63177983742345e-13\n",
            "||QR - A||_F =3.867117111147723e-12\n",
            "USING np.linalg.qr()\n",
            "||Q^TQ-I||_F = 7.152893719812505e-15\n",
            "||QR - A||_F =1.54580529232833e-11\n",
            "A dim: 42 times 42\n",
            "REPORT IMPLEMENTATION\n",
            "||Q^TQ-I||_F = 3.1419201757218816e-13\n",
            "||QR - A||_F =1.504078003597897e-12\n",
            "USING np.linalg.qr()\n",
            "||Q^TQ-I||_F = 3.4985811686944474e-15\n",
            "||QR - A||_F =5.524992988838227e-12\n",
            "A dim: 31 times 31\n",
            "REPORT IMPLEMENTATION\n",
            "||Q^TQ-I||_F = 4.956161028484966e-14\n",
            "||QR - A||_F =9.91623045653687e-13\n",
            "USING np.linalg.qr()\n",
            "||Q^TQ-I||_F = 2.9903804354167748e-15\n",
            "||QR - A||_F =4.056376414939797e-12\n",
            "A dim: 94 times 94\n",
            "REPORT IMPLEMENTATION\n",
            "||Q^TQ-I||_F = 4.330113496240048e-13\n",
            "||QR - A||_F =4.163053524517062e-12\n",
            "USING np.linalg.qr()\n",
            "||Q^TQ-I||_F = 7.54420715483119e-15\n",
            "||QR - A||_F =1.6292643343046567e-11\n",
            "A dim: 93 times 93\n",
            "REPORT IMPLEMENTATION\n",
            "||Q^TQ-I||_F = 1.9040247115778828e-13\n",
            "||QR - A||_F =4.037575341448104e-12\n",
            "USING np.linalg.qr()\n",
            "||Q^TQ-I||_F = 7.355569501707817e-15\n",
            "||QR - A||_F =1.5993828896268073e-11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzLzSyUfvWVD",
        "colab_type": "text"
      },
      "source": [
        "### **Direct Solver**\n",
        "\n",
        "To test the implemented direct solver we inspect the residual $\\Vert Ax - b\\Vert$ and $\\Vert x-y \\Vert$ where $y$ is a manufactured solution with $b=Ay$.\n",
        "\n",
        "We start by inspecting the residual for the solution to 10 randomized systems of equations, and comparing to the np.linalg.solve() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zM76YLIv7o1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "313f852d-8986-4b64-a17b-bfb46fc513d4"
      },
      "source": [
        "for i in range(10):\n",
        "  m = random.randint(1,100)\n",
        "  A = 1000 * np.random.random_sample((m, m)) - 500 # [-500,500]\n",
        "  b = 1000 * np.random.random_sample((m,1)) - 500\n",
        "  x = direct_solver(A,b)\n",
        "  x2 = np.linalg.solve(A,b)\n",
        "  print(\"A dim: \" + str(m) + \" times \" + str(m))\n",
        "  print(\"REPORT IMPLEMENTATIOn\")\n",
        "  print(\"||Ax-b|| = \" + str(np.linalg.norm(A.dot(x)- b)))\n",
        "  print(\"USING np.linalg.solve()\")\n",
        "  print(\"||Ax-b|| = \" + str(np.linalg.norm(A.dot(x2)- b)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A dim: 9 times 9\n",
            "REPORT IMPLEMENTATIOn\n",
            "||Ax-b|| = 2.412875915867585e-12\n",
            "USING np.linalg.solve()\n",
            "||Ax-b|| = 1.4224526086681128e-12\n",
            "A dim: 63 times 63\n",
            "REPORT IMPLEMENTATIOn\n",
            "||Ax-b|| = 2.789310328345715e-10\n",
            "USING np.linalg.solve()\n",
            "||Ax-b|| = 6.553890896393737e-11\n",
            "A dim: 19 times 19\n",
            "REPORT IMPLEMENTATIOn\n",
            "||Ax-b|| = 1.6177920623201438e-12\n",
            "USING np.linalg.solve()\n",
            "||Ax-b|| = 1.1058930600906773e-12\n",
            "A dim: 39 times 39\n",
            "REPORT IMPLEMENTATIOn\n",
            "||Ax-b|| = 5.337503199215519e-11\n",
            "USING np.linalg.solve()\n",
            "||Ax-b|| = 3.342064326417041e-11\n",
            "A dim: 9 times 9\n",
            "REPORT IMPLEMENTATIOn\n",
            "||Ax-b|| = 3.152920801928444e-13\n",
            "USING np.linalg.solve()\n",
            "||Ax-b|| = 2.773848179905249e-13\n",
            "A dim: 89 times 89\n",
            "REPORT IMPLEMENTATIOn\n",
            "||Ax-b|| = 3.2928507606123625e-11\n",
            "USING np.linalg.solve()\n",
            "||Ax-b|| = 2.1044223507251803e-11\n",
            "A dim: 39 times 39\n",
            "REPORT IMPLEMENTATIOn\n",
            "||Ax-b|| = 1.0080175068195804e-11\n",
            "USING np.linalg.solve()\n",
            "||Ax-b|| = 1.3875187745221335e-11\n",
            "A dim: 91 times 91\n",
            "REPORT IMPLEMENTATIOn\n",
            "||Ax-b|| = 2.5224271835585427e-10\n",
            "USING np.linalg.solve()\n",
            "||Ax-b|| = 7.368904529690947e-11\n",
            "A dim: 26 times 26\n",
            "REPORT IMPLEMENTATIOn\n",
            "||Ax-b|| = 3.2808767429068812e-12\n",
            "USING np.linalg.solve()\n",
            "||Ax-b|| = 2.2157657454266023e-12\n",
            "A dim: 61 times 61\n",
            "REPORT IMPLEMENTATIOn\n",
            "||Ax-b|| = 5.598305747801994e-12\n",
            "USING np.linalg.solve()\n",
            "||Ax-b|| = 5.522375442952451e-12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HigDXH2xJEI",
        "colab_type": "text"
      },
      "source": [
        "To test a manufactured solution we use the system,\n",
        "$$\n",
        "\\begin{pmatrix} 1 & 2 & 3 \\\\ 1 & 0 & 3 \\\\ 0 & 4 & 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\n",
        "$$\n",
        "which has the solution,\n",
        "$$\n",
        "\\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} -13 \\\\ -\\frac{1}{2} \\\\ 5 \\end{pmatrix}$$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfzCixvfzY2D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "361cd53f-c023-462c-f9eb-c4378cb26c72"
      },
      "source": [
        "A = np.array([[1,2,3],[1,0,3],[0,4,1]])\n",
        "b = np.array([[1],[2],[3]])\n",
        "y = np.array([[-13],[-0.5],[5]])\n",
        "x = direct_solver(A,b)\n",
        "print(x)\n",
        "print(np.linalg.norm(x - y))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-13. ]\n",
            " [ -0.5]\n",
            " [  5. ]]\n",
            "2.094764613337708e-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pQ0VY2fyItQ",
        "colab_type": "text"
      },
      "source": [
        "### **Least Squarres**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_DJCCeP0NZO",
        "colab_type": "text"
      },
      "source": [
        "To test our least squares implementation we will again inspect the residual $\\Vert Ax - b \\Vert$ for 10 randomized least square problems, and compare against calculating the normal equations and solving using np.linalg."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDKq9AFb0hDQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4c3614e-1e1f-44af-8173-b58b747b8aa8"
      },
      "source": [
        "for i in range(20):\n",
        "  m = random.randint(7,25)\n",
        "  n = random.randint(5, m-1)\n",
        "  A = 1000 * np.random.random_sample((m, n)) - 500 # [-500,500]\n",
        "  b = 1000 * np.random.random_sample((m,1)) - 500\n",
        "  x = least_squares(A,b)\n",
        "  x2 = np.linalg.solve(np.transpose(A).dot(A),np.transpose(A).dot(b))\n",
        "  print(\"A dim: \" + str(m) + \" times \" + str(n))\n",
        "  print(\" A dim diff = \" + str(m-n))\n",
        "  print(\"REPORT IMPLEMENTATION\")\n",
        "  print(\"||Ax - b|| = \"  + str(np.linalg.norm(A.dot(x) - b)))\n",
        "  print(\"USING np.linalg\")\n",
        "  print(\"||Ax - b|| = \"  + str(np.linalg.norm(A.dot(x2) - b)))\n",
        "  print(\"DIFF: \" + str(np.abs(np.linalg.norm(A.dot(x) - b)- np.linalg.norm(A.dot(x2) - b))))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A dim: 15 times 5\n",
            " A dim diff = 10\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 908.4097805155958\n",
            "USING np.linalg\n",
            "||Ax - b|| = 908.4097805155959\n",
            "DIFF: 1.1368683772161603e-13\n",
            "A dim: 7 times 5\n",
            " A dim diff = 2\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 25.550174719595134\n",
            "USING np.linalg\n",
            "||Ax - b|| = 25.550174719595176\n",
            "DIFF: 4.263256414560601e-14\n",
            "A dim: 16 times 14\n",
            " A dim diff = 2\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 216.99672360820995\n",
            "USING np.linalg\n",
            "||Ax - b|| = 216.99672360821\n",
            "DIFF: 5.684341886080802e-14\n",
            "A dim: 19 times 18\n",
            " A dim diff = 1\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 172.2324629895131\n",
            "USING np.linalg\n",
            "||Ax - b|| = 172.23246298951304\n",
            "DIFF: 5.684341886080802e-14\n",
            "A dim: 14 times 10\n",
            " A dim diff = 4\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 576.1907877237278\n",
            "USING np.linalg\n",
            "||Ax - b|| = 576.1907877237279\n",
            "DIFF: 1.1368683772161603e-13\n",
            "A dim: 9 times 7\n",
            " A dim diff = 2\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 386.13386804445196\n",
            "USING np.linalg\n",
            "||Ax - b|| = 386.133868044452\n",
            "DIFF: 5.684341886080802e-14\n",
            "A dim: 22 times 20\n",
            " A dim diff = 2\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 23.625676265174512\n",
            "USING np.linalg\n",
            "||Ax - b|| = 23.625676265174494\n",
            "DIFF: 1.7763568394002505e-14\n",
            "A dim: 11 times 10\n",
            " A dim diff = 1\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 162.81471975138686\n",
            "USING np.linalg\n",
            "||Ax - b|| = 162.81471975138697\n",
            "DIFF: 1.1368683772161603e-13\n",
            "A dim: 23 times 7\n",
            " A dim diff = 16\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 1045.521988029206\n",
            "USING np.linalg\n",
            "||Ax - b|| = 1045.521988029206\n",
            "DIFF: 0.0\n",
            "A dim: 7 times 5\n",
            " A dim diff = 2\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 565.3183485645238\n",
            "USING np.linalg\n",
            "||Ax - b|| = 565.3183485645238\n",
            "DIFF: 0.0\n",
            "A dim: 22 times 17\n",
            " A dim diff = 5\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 764.2466710474993\n",
            "USING np.linalg\n",
            "||Ax - b|| = 764.246671047499\n",
            "DIFF: 2.2737367544323206e-13\n",
            "A dim: 11 times 9\n",
            " A dim diff = 2\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 431.2892150381377\n",
            "USING np.linalg\n",
            "||Ax - b|| = 431.28921503813774\n",
            "DIFF: 5.684341886080802e-14\n",
            "A dim: 11 times 7\n",
            " A dim diff = 4\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 399.4846682415824\n",
            "USING np.linalg\n",
            "||Ax - b|| = 399.4846682415825\n",
            "DIFF: 5.684341886080802e-14\n",
            "A dim: 13 times 6\n",
            " A dim diff = 7\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 534.6836985587123\n",
            "USING np.linalg\n",
            "||Ax - b|| = 534.6836985587123\n",
            "DIFF: 0.0\n",
            "A dim: 7 times 6\n",
            " A dim diff = 1\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 370.75821367419667\n",
            "USING np.linalg\n",
            "||Ax - b|| = 370.7582136741966\n",
            "DIFF: 5.684341886080802e-14\n",
            "A dim: 17 times 10\n",
            " A dim diff = 7\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 686.4382481749457\n",
            "USING np.linalg\n",
            "||Ax - b|| = 686.4382481749457\n",
            "DIFF: 0.0\n",
            "A dim: 23 times 17\n",
            " A dim diff = 6\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 628.2369557635792\n",
            "USING np.linalg\n",
            "||Ax - b|| = 628.2369557635792\n",
            "DIFF: 0.0\n",
            "A dim: 9 times 6\n",
            " A dim diff = 3\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 519.0106300704475\n",
            "USING np.linalg\n",
            "||Ax - b|| = 519.0106300704475\n",
            "DIFF: 0.0\n",
            "A dim: 15 times 7\n",
            " A dim diff = 8\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 696.6819841023523\n",
            "USING np.linalg\n",
            "||Ax - b|| = 696.6819841023524\n",
            "DIFF: 1.1368683772161603e-13\n",
            "A dim: 8 times 5\n",
            " A dim diff = 3\n",
            "REPORT IMPLEMENTATION\n",
            "||Ax - b|| = 492.1241838489875\n",
            "USING np.linalg\n",
            "||Ax - b|| = 492.1241838489875\n",
            "DIFF: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m",
        "colab_type": "text"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe",
        "colab_type": "text"
      },
      "source": [
        "All of the implemented algorithms appear to be working properly.\n",
        "\n",
        "**Sparse Matrix-Vector Product**: The implementation passed all tests against the np.dot() function. \n",
        "\n",
        "**Gram-Schmidt QR Factorization**: The residual was quite small for all tests, and R appears to always be upper-triangular. It was quite surprising that the residuals was so small even though the values of the matrices were in the range of $[-500,500)$, as the classical Gram-Schmidt QR factorization has some problems with numerical stability (section 5.3). However, the test cases did probably not cover such instances. The np.linalg.qr() function appear to have a slight edge compared to our implementation.\n",
        "\n",
        "**Direct Solver**: The residual was quite small for all tests, so the algorithm appear to perform quite well. The residual for our implementation is also very close the the residual using np.linalg.solve().\n",
        "\n",
        "**Least Squares**: Here we got a much larger residual then for the direct solver, which is expected as the system is overdecided and we are projecting $b$ onto $A$. It appears that the residual is increasing as the difference in dimension between $A$ and $b$ increase, this is likely due to the projection, but could also be due to numerical instability as the system size increases. Surprisingly our implementation performs almost identically to using numpy for all calculations."
      ]
    }
  ]
}