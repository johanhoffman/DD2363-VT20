{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "timaslj-lab-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363-VT20/blob/timaslj2/timaslj_lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7",
        "colab_type": "text"
      },
      "source": [
        "# **Lab 2: Matrix factorization**\n",
        "**Timas Ljungdahl**\n",
        "\n",
        "In collaboration with **Kristoffer Almroth**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm",
        "colab_type": "text"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL",
        "colab_type": "text"
      },
      "source": [
        "In this report, 4 different algorithms were implemented and tested. The algorithms were sparse matrix-vector multiplication for matrices of CRS format, modified Gram-Schmidt iteration for QR-factorization, backwards substitution for solving $Rx = b$ and finally QR eigenvalue algorithm for finding the eigenvalues and eigenvectors for a matrix $A$. All algorithms were implemented and tested with random data and generally generated results with around 10 decimal precision. The results are probably highly affected by floating point error that occur when continuously adding and subtracting numbers of different magnitude. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3",
        "colab_type": "text"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo",
        "colab_type": "text"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is distributed under a certain license. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab_type": "code",
        "outputId": "f74fa781-413b-41e7-a2ec-1bba2288ad4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2019 Johan Hoffman (jhoffman@kth.se)\n",
        "\n",
        "# This file is part of the course DD2363 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh",
        "colab_type": "text"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa",
        "colab_type": "text"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import unittest\n",
        "import random\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6",
        "colab_type": "text"
      },
      "source": [
        "In this report, systems of linear equations are investigated of the form $Ax = b$\n",
        "where we want to solve for $x$ given a matrix $A$ and vector $b$. A system of linear equations has an exact solution if A is nonsingular, meaning that A has an inverse, since then $x = A^{-1}b$. In this report we assume that A is a square, nonsingular matrix. \n",
        "\n",
        "The direct solution methods implemented in this report are based on factorization of A into easily invertable matrices - diagonal, orthonormal and triangular matrices. The problem of solving for $x$ is summerized below.\n",
        "\n",
        "$Ax = b$, solve for $x$\n",
        "1. Factorize $A$ into $QR$, where $Q$ is an orthonormal matrix and $R$ is an upper triangular matrix. This gives us $QRx = b$.\n",
        "2. Since $Q$ is orthonormal $Q^{-1} = Q^T$. Orthonormal, means that each column vector in the matrix is normalized and orthogonal to each other. Multiplying $Q^{-1}$ on the left on each side, we get $Q^{-1}QRx = Q^{-1}b => Rx = Q^Tb$.\n",
        "3. We can now easily solve $Rx = Q^Tb$ \n",
        "\n",
        "Apart from solving systems of equations, sparse matrix-vector multiplication was implemented for sparse matrices of compressed row storage(CRS)format. Instead of storing all values in the matrix, only the nonzero values are stored in an array ***v***, along with the two index arrays ***col_idx*** and ***row_ptr***. There is an entry in the ***col_idx*** array for each value to indicate the column of the value in the original matrix. The ***row_ptr*** array consists of the index in ***v*** where each row starts. \n",
        "\n",
        "The QR eigenvalue algorithm was also implementet which for a real symmetric matrix $A$, returns a unitary matrix $U$ with the eigenvectors of $A$ as column vectors and a upper triangular matrix $T$ with eigenvalues of $A$ in the diagonal. It finds the a Schur factorization such that $A = UTU^{*}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeFO9QMeUOAu",
        "colab_type": "text"
      },
      "source": [
        "# **Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx",
        "colab_type": "text"
      },
      "source": [
        "To achieve the goal of solving the system of equations, the first step is to implement an effecient factorization method. In this report, a modified version of Gram-Schmidt iteration is implemented. This method recursively computes an orthonormal matrix $Q$ from $A$ by taking each column vector in $A$ and subtracting its projection onto the already computed orthonormal space in order to compute a new perpendicular vector which is then added to the set of orthonormal vectors. $R$ is then computed as $R = Q^TA$.  \n",
        "\n",
        "When $Q$ and $R$ have been computed, $b$ is multiplied by $Q^T$ on the left to form a new vector $b_{prim}$ so that $Rx = b_{prim}$. This new system of equations can now be solved with backwards subsitution as $R$ is upper triangular. This is possible since $x_n = b_n/a_{nn}$ which can then be substituted in order to solve for $x_{n-1} = (b_{n-1} - a_{(n-1)(n)}x_n)/a_{(n-1)(n-1)}$. This can be written as:\n",
        "\n",
        "$x_i = (b_i - \\sum_{j=i+1}^{n}a_{ij}x_j)/a_{ii} $\n",
        "\n",
        "In order to implement these algorithms, pseudocode from the lecture notes provided in class were followed. \n",
        "\n",
        "To test the QR factorization method,random $A \\epsilon \\mathbb{R}^{n \\times n}$ were generated with imported numpy methods. The output $Q$ and $R$ of the algorithm were then multiplied together again and asserted to be equal to $A$.\n",
        "\n",
        "To test the direct solver method, random $A \\epsilon \\mathbb{R}^{n \\times n}$ and $b \\epsilon \\mathbb{R}^{n}$ were generated and the residuals $|| Ax-b ||$ and $|| x-y ||$ were asserted to be equal to zero. \n",
        "\n",
        "To test the QR eigenvalue algorithm, a random real symmetric matrix A was generated and $det(A - \\lambda_iI)$ was asserted to be equal to 0. $||Av_i - \\lambda_iv_i||$ was also asserted to be equal to 0. The tests follow the definition of eigenvectors and eigenvalues: $Av = \\lambda v$ where $v$ is an eigenvector and $\\lambda$ is an eigenvalue. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_",
        "colab_type": "text"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd",
        "colab_type": "text"
      },
      "source": [
        "### **Sparse matrix vector multiplication**\n",
        "\n",
        "This algorithm was tested by comparing the result with the result of numpy's matmul function. It was tested on sparse matrices, that were generated by setting the majority of the elements to $0$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FOdVWesDOI9",
        "colab_type": "code",
        "outputId": "d1da5cfe-9061-4f0a-e8fc-1289f4f0f933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "def sparse_matrix_vector_product(x, v, col_idx, row_ptr):\n",
        "  assert x.size == (len(row_ptr)-1)\n",
        "\n",
        "  product = np.zeros(len(row_ptr)-1)\n",
        "\n",
        "  for row in range(len(row_ptr)-1):\n",
        "    res = 0\n",
        "    for i in range(row_ptr[row], row_ptr[row+1]):\n",
        "      res += v[i] * x[col_idx[i]]\n",
        "    product[row] = res\n",
        "  \n",
        "  return product\n",
        "\n",
        "class Test(unittest.TestCase):\n",
        "  \n",
        "  def test_illegal_dimensions(self):\n",
        "    x = np.array([1,2])\n",
        "    v = [3,2,2,2,1,1,3,2,1,2,3]\n",
        "    col_idx = [1,2,4,2,3,3,3,4,5,5,6]\n",
        "    row_ptr = [1,4,6,7,9,10,12]\n",
        "    with self.assertRaises(AssertionError):\n",
        "      sparse_matrix_vector_product(x,v, col_idx,row_ptr)\n",
        "\n",
        "  def test_against_dense_product(self):\n",
        "    A = np.array([[3,2,0,2,0,0],\n",
        "                  [0,2,1,0,0,0],\n",
        "                  [0,0,1,0,0,0],\n",
        "                  [0,0,3,2,0,0],\n",
        "                  [0,0,0,0,1,0],\n",
        "                  [0,0,0,0,2,3]])\n",
        "    x = np.array([2,1,2,4,1,3])\n",
        "    v = [3,2,2,2,1,1,3,2,1,2,3]\n",
        "    col_idx = [0,1,3,1,2,2,2,3,4,4,5]\n",
        "    row_ptr = [0,3,5,6,8,9,11]\n",
        "    np.testing.assert_array_equal(np.matmul(A,x), sparse_matrix_vector_product(x,v,col_idx,row_ptr))\n",
        "\n",
        "  def test_random_sparse_matrix(self):\n",
        "    for i in range(100):\n",
        "      size = random.randint(2, 100) \n",
        "      A = np.random.rand(size, size)\n",
        "      x = np.random.rand(size)\n",
        "      v = []\n",
        "      col_idx = []\n",
        "      row_ptr = []\n",
        "      # make the random matrix sparse, on average 2/3 of matrix are zeros\n",
        "      for i in range(size):\n",
        "        seen = False\n",
        "        nr_of_zeros = 0\n",
        "        for j in range(size):\n",
        "          if not(random.randint(0,3) == 0) and nr_of_zeros < size-1:\n",
        "            A[i,j] = 0\n",
        "            nr_of_zeros += 1\n",
        "          else:\n",
        "            if not(seen):\n",
        "              row_ptr.append(len(v))\n",
        "              seen = True\n",
        "            v.append(A[i,j])\n",
        "            col_idx.append(j)\n",
        "      row_ptr.append(len(v))\n",
        "\n",
        "      np.testing.assert_array_almost_equal(np.matmul(A,x), sparse_matrix_vector_product(x,v,col_idx,row_ptr), 14)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.854s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VLH_o83wKCi",
        "colab_type": "text"
      },
      "source": [
        "### **QR-factorization**\n",
        "\n",
        "The algorithm was tested by checking that every column vector in the output matrix $Q$ has the norm $1$ and that all column vector are orthogonal to each other, meaning that the dot-product is $0$. If this is the case $Q$ is indeed orthonormal. The matrix $R$ was also tested to check that it indeed is an upper triangular matrix. The output $QR$ was also tested to see that the product is the same as the input matrix $A$. This was tested by comparing the Frobenius norm of the the matrices. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDPNsceUTqW1",
        "colab_type": "code",
        "outputId": "847fb571-6e1e-42c9-b61c-e363afd0c54b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "def qr_factorization(A):\n",
        "  v = np.copy(A)\n",
        "  n = A.shape[0]\n",
        "  R = np.zeros((n,n))\n",
        "  Q = np.zeros((n,n))\n",
        "  for i in range(n):\n",
        "    R[i,i] = np.sqrt(np.dot(v[:,i],v[:,i])) #norm(v)\n",
        "    Q[:,i] = v[:,i]/R[i,i] #normalize\n",
        "    for j in range(i+1, n):\n",
        "      R[i,j] = np.dot(Q[:,i], v[:,j]) #qA\n",
        "      v[:,j] = np.subtract(v[:,j], R[i,j]*Q[:,i]) #update orthogonal set v\n",
        "  return Q,R\n",
        "\n",
        "def frobenius_norm(A):\n",
        "  res = 0\n",
        "  for i in A:\n",
        "    for j in i:\n",
        "      res += j*j\n",
        "  return np.sqrt(res)\n",
        "\n",
        "class Test(unittest.TestCase):\n",
        "\n",
        "  def test_R_upper_triangular(self):\n",
        "    size = random.randint(2, 100) \n",
        "    A = np.random.rand(size, size)\n",
        "    _, R = qr_factorization(A)\n",
        "\n",
        "    for i in range(size):\n",
        "      for j in range(0, i):\n",
        "        self.assertEqual(R[i,j], 0)\n",
        "  \n",
        "  '''In order for Q to be orthonormal, all column vectors must be normalized\n",
        "  and orthogonal to one another\n",
        "  '''\n",
        "  def test_Q_orthonormal(self):\n",
        "    for n in range(100):\n",
        "      size = random.randint(2,100) \n",
        "      A = np.random.rand(size, size)\n",
        "      Q, _ = qr_factorization(A)\n",
        "      for i in range(size):\n",
        "        # norm == 1\n",
        "        self.assertAlmostEqual(np.sqrt(np.dot(Q[:,i],Q[:,i])), 1, 15)\n",
        "        for j in range(i+1,size):\n",
        "          #orthogonal\n",
        "          self.assertAlmostEqual(np.dot(Q[:,i], Q[:,j]), 0, 10)\n",
        "        \n",
        "  def test_Q_R_equals_A(self):\n",
        "    for n in range(100):\n",
        "      size = random.randint(2,100) \n",
        "      A = np.random.rand(size, size)\n",
        "      Q, R = qr_factorization(A)\n",
        "\n",
        "      np.testing.assert_array_almost_equal(np.matmul(Q,R), A, 14)\n",
        "  \n",
        "  def test_frobenius_norms(self):\n",
        "    for n in range(100):\n",
        "      size = random.randint(2,100) \n",
        "      A = np.random.rand(size, size)\n",
        "      Q, R = qr_factorization(A)\n",
        "      #|| QR-A ||_F\n",
        "      self.assertAlmostEqual(frobenius_norm(np.matmul(Q,R)) - frobenius_norm(A), 0, 11)\n",
        "\n",
        "      Q_trans = np.transpose(Q)\n",
        "      #|| Q^TQ ||\n",
        "      self.assertAlmostEqual(frobenius_norm(np.matmul(Q_trans, Q)), np.sqrt(size), 14)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "....\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 7.398s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S_VNBmxweTV",
        "colab_type": "text"
      },
      "source": [
        "### **Backward substitution**\n",
        "\n",
        "The backward substitution algorithm was tested with randomly generated upper triangular matrices $A$ and vectors $b$ to see that $Ax_{output} = b$. The direct solver was tested by checking that the residual $||Ax-b|| = 0$   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D6N9FL9AolZ",
        "colab_type": "code",
        "outputId": "2bcda418-5c1b-42df-a3f7-d0d2cebff4b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "def backward_subsitution(A,b):\n",
        "  n = b.shape[0]\n",
        "  x = np.zeros(n)\n",
        "  x[n-1] = b[n-1]/A[n-1,n-1]\n",
        "  for i in range(n-2, -1, -1):\n",
        "    res = 0\n",
        "    for j in range(i+1, n):\n",
        "      res += A[i,j]*x[j]\n",
        "    x[i] = (b[i] - res)/A[i,i]\n",
        "  return x\n",
        "\n",
        "def direct_solver(A, b):\n",
        "  Q, R = qr_factorization(A)\n",
        "  b_prim = np.zeros(Q.shape[0])\n",
        "\n",
        "  #Q^t * b\n",
        "  for j in range(Q.shape[0]):\n",
        "    b_prim[j] = np.dot(Q[:,j],b)\n",
        "\n",
        "  return backward_subsitution(R, b_prim)\n",
        "\n",
        "class Test(unittest.TestCase):\n",
        "\n",
        "  def test_backward_sub(self):\n",
        "    for n in range(100):\n",
        "      size = random.randint(2,100) \n",
        "      A = np.random.rand(size, size)\n",
        "      b = np.random.rand(size)\n",
        "      #get random upper triangular matrix\n",
        "      _, R = qr_factorization(A)\n",
        "\n",
        "      x = backward_subsitution(R, b)\n",
        "      \n",
        "      np.testing.assert_array_almost_equal(np.matmul(R,x), b, 11)\n",
        "\n",
        "  def test_residuals(self):\n",
        "    for n in range(100):\n",
        "      size = random.randint(2,100) \n",
        "      A = np.random.rand(size, size)\n",
        "      b = np.random.rand(size)\n",
        "\n",
        "      y = direct_solver(A, b)\n",
        "\n",
        "      res_vec = np.matmul(A,y)-b\n",
        "      \n",
        "      #||Ax-b||\n",
        "      self.assertAlmostEqual(np.sqrt(np.dot(res_vec,res_vec)), 0, 10)\n",
        "\n",
        "      x = np.linalg.solve(A, b)\n",
        "      diff_vec = x-y\n",
        "\n",
        "      #||x-y||\n",
        "      self.assertAlmostEqual(np.sqrt(np.dot(diff_vec,diff_vec)), 0, 7)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "..\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 3.597s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbPzq1tDwmaR",
        "colab_type": "text"
      },
      "source": [
        "### **QR eigenvalue algorithm**\n",
        "\n",
        "The returned matrix $A$ has the eigenvalues of the original symmetric input $A_{sym}$ in the diagonal and the matrix $U$ has the eigenvectors as column vectors. The algorithm is therefore tested by checking that $||A_{sym}U_{i} - A_{ii}\\times U_{i}|| = 0$ where $U_i$ are the eigenvectors of $A_{sym}$ and $A_{ii}$ are the eigenvalues. This test is based on the definitions of eigenvalues and eigenvectors. $det(A_{sym}-A{ii}\\times I)$ is also asserted to $0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK1E5cK7HXqd",
        "colab_type": "code",
        "outputId": "67ea3450-7dbc-46a7-a831-238af7936861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "def qr_algorithm(A):\n",
        "  n = A.shape[0]\n",
        "  U = np.identity(n)\n",
        "  for k in range(10000):\n",
        "    Q, R = qr_factorization(A)\n",
        "    A = np.matmul(R, Q)\n",
        "    U = np.matmul(U, Q)\n",
        "  return A, U\n",
        "\n",
        "class Test(unittest.TestCase):\n",
        "\n",
        "  def test_determinant(self):\n",
        "    for n in range(100):\n",
        "      size = random.randint(2,5)\n",
        "      sym_A = np.zeros((size,size))\n",
        "\n",
        "      for i in range(size):\n",
        "        for j in range(size):\n",
        "          rand_int = random.randint(0,50)\n",
        "          sym_A[i,j] = rand_int\n",
        "          sym_A[j,i] = rand_int\n",
        "      A, U = qr_algorithm(sym_A)\n",
        "      \n",
        "      id_matrix = np.identity(size)\n",
        "      for i in range(size):\n",
        "        res = np.matmul(sym_A, U[:,i]) - A[i,i]*U[:,i]\n",
        "        self.assertAlmostEqual(np.sqrt(np.dot(res,res)), 0, 11)\n",
        "        self.assertAlmostEqual(np.linalg.det(sym_A - (A[i,i]*id_matrix)), 0, 3)\n",
        "\n",
        "    #for i in range(size):\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 76.072s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m",
        "colab_type": "text"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe",
        "colab_type": "text"
      },
      "source": [
        "All algorithms were implemented and tested with random data several times. The precision of the algorithms differs but generally the output had a precision of around 10 decimals. The precision is probabaly affected by floating point errors that occur when adding and subtracting numbers of different magnitude. The modified Graham-Schmidt iteration, however, mitigates the absorption effect of floating point addition as the orthonormal set is generated recursively and does not rely on the summation of a large set of numbers.    "
      ]
    }
  ]
}